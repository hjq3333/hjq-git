2025-08-04 10:26:29,973 - INFO - 发现 27 张待同步表
2025-08-04 10:26:30,376 - ERROR - 同步表 ads_instore_path_analysis_pc 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:30,376 - ERROR - 表 ads_instore_path_analysis_pc 同步失败，跳过继续处理其他表
2025-08-04 10:26:30,627 - ERROR - 同步表 ads_instore_path_analysis_wireless 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:30,627 - ERROR - 表 ads_instore_path_analysis_wireless 同步失败，跳过继续处理其他表
2025-08-04 10:26:30,818 - ERROR - 同步表 ads_pc_entry_indicator 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:30,819 - ERROR - 表 ads_pc_entry_indicator 同步失败，跳过继续处理其他表
2025-08-04 10:26:31,015 - ERROR - 同步表 ads_pc_entry_page_type_top20 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:31,015 - ERROR - 表 ads_pc_entry_page_type_top20 同步失败，跳过继续处理其他表
2025-08-04 10:26:31,218 - ERROR - 同步表 ads_shop_page_analysis_pc 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:31,218 - ERROR - 表 ads_shop_page_analysis_pc 同步失败，跳过继续处理其他表
2025-08-04 10:26:31,394 - ERROR - 同步表 ads_shop_page_analysis_wireless 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:31,395 - ERROR - 表 ads_shop_page_analysis_wireless 同步失败，跳过继续处理其他表
2025-08-04 10:26:31,560 - ERROR - 同步表 ads_wireless_entry_indicator 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:31,561 - ERROR - 表 ads_wireless_entry_indicator 同步失败，跳过继续处理其他表
2025-08-04 10:26:31,757 - ERROR - 同步表 dwd_instore_path_pc 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:31,758 - ERROR - 表 dwd_instore_path_pc 同步失败，跳过继续处理其他表
2025-08-04 10:26:31,961 - ERROR - 同步表 dwd_instore_path_wireless 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:31,962 - ERROR - 表 dwd_instore_path_wireless 同步失败，跳过继续处理其他表
2025-08-04 10:26:32,122 - ERROR - 同步表 dwd_pc_entry_detail 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:32,123 - ERROR - 表 dwd_pc_entry_detail 同步失败，跳过继续处理其他表
2025-08-04 10:26:32,319 - ERROR - 同步表 dwd_shop_page_visit_detail_pc 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:32,319 - ERROR - 表 dwd_shop_page_visit_detail_pc 同步失败，跳过继续处理其他表
2025-08-04 10:26:32,473 - ERROR - 同步表 dwd_shop_page_visit_detail_wireless 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:32,473 - ERROR - 表 dwd_shop_page_visit_detail_wireless 同步失败，跳过继续处理其他表
2025-08-04 10:26:32,647 - ERROR - 同步表 dwd_wireless_entry_detail 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:32,647 - ERROR - 表 dwd_wireless_entry_detail 同步失败，跳过继续处理其他表
2025-08-04 10:26:32,884 - ERROR - 同步表 dws_instore_path_summary_pc 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:32,884 - ERROR - 表 dws_instore_path_summary_pc 同步失败，跳过继续处理其他表
2025-08-04 10:26:33,061 - ERROR - 同步表 dws_instore_path_summary_wireless 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:33,062 - ERROR - 表 dws_instore_path_summary_wireless 同步失败，跳过继续处理其他表
2025-08-04 10:26:33,214 - ERROR - 同步表 dws_pc_entry_daily 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:33,214 - ERROR - 表 dws_pc_entry_daily 同步失败，跳过继续处理其他表
2025-08-04 10:26:33,376 - ERROR - 同步表 dws_pc_entry_multi_period 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:33,376 - ERROR - 表 dws_pc_entry_multi_period 同步失败，跳过继续处理其他表
2025-08-04 10:26:33,555 - ERROR - 同步表 dws_pc_entry_page_type_analysis 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:33,556 - ERROR - 表 dws_pc_entry_page_type_analysis 同步失败，跳过继续处理其他表
2025-08-04 10:26:33,705 - ERROR - 同步表 dws_pc_source_page_analysis 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:33,705 - ERROR - 表 dws_pc_source_page_analysis 同步失败，跳过继续处理其他表
2025-08-04 10:26:33,881 - ERROR - 同步表 dws_shop_page_daily_summary_pc 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:33,881 - ERROR - 表 dws_shop_page_daily_summary_pc 同步失败，跳过继续处理其他表
2025-08-04 10:26:34,051 - ERROR - 同步表 dws_shop_page_daily_summary_wireless 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:34,051 - ERROR - 表 dws_shop_page_daily_summary_wireless 同步失败，跳过继续处理其他表
2025-08-04 10:26:34,203 - ERROR - 同步表 dws_wireless_entry_daily 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:34,203 - ERROR - 表 dws_wireless_entry_daily 同步失败，跳过继续处理其他表
2025-08-04 10:26:34,377 - ERROR - 同步表 dws_wireless_entry_multi_period 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:34,377 - ERROR - 表 dws_wireless_entry_multi_period 同步失败，跳过继续处理其他表
2025-08-04 10:26:34,580 - ERROR - 同步表 ods_instore_path 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:34,580 - ERROR - 表 ods_instore_path 同步失败，跳过继续处理其他表
2025-08-04 10:26:34,768 - ERROR - 同步表 ods_page_visit_rank 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:34,768 - ERROR - 表 ods_page_visit_rank 同步失败，跳过继续处理其他表
2025-08-04 10:26:34,934 - ERROR - 同步表 ods_pc_or_wireless_entry 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:34,934 - ERROR - 表 ods_pc_or_wireless_entry 同步失败，跳过继续处理其他表
2025-08-04 10:26:35,080 - ERROR - 同步表 ods_shop_page_visit_detail 结构失败: (1060, "Duplicate column name 'dt'")
2025-08-04 10:26:35,080 - ERROR - 表 ods_shop_page_visit_detail 同步失败，跳过继续处理其他表
2025-08-04 10:26:35,242 - INFO - Closing down clientserver connection
2025-08-04 10:28:27,563 - INFO - 发现 27 张待同步表
2025-08-04 10:28:27,780 - WARNING - 表 ads_instore_path_analysis_pc 中发现重复列: dt，已跳过
2025-08-04 10:28:27,899 - INFO - 表 ads_instore_path_analysis_pc 结构同步成功
2025-08-04 10:28:30,699 - INFO - 表 ads_instore_path_analysis_pc 数据同步完成，记录数: 29
2025-08-04 10:28:30,912 - WARNING - 表 ads_instore_path_analysis_wireless 中发现重复列: dt，已跳过
2025-08-04 10:28:31,139 - INFO - 表 ads_instore_path_analysis_wireless 结构同步成功
2025-08-04 10:28:31,793 - INFO - 表 ads_instore_path_analysis_wireless 数据同步完成，记录数: 29
2025-08-04 10:28:31,880 - WARNING - 表 ads_pc_entry_indicator 中发现重复列: dt，已跳过
2025-08-04 10:28:31,972 - INFO - 表 ads_pc_entry_indicator 结构同步成功
2025-08-04 10:28:32,182 - INFO - 表 ads_pc_entry_indicator 数据同步完成，记录数: 9
2025-08-04 10:28:32,271 - WARNING - 表 ads_pc_entry_page_type_top20 中发现重复列: dt，已跳过
2025-08-04 10:28:32,376 - INFO - 表 ads_pc_entry_page_type_top20 结构同步成功
2025-08-04 10:28:32,622 - INFO - 表 ads_pc_entry_page_type_top20 数据同步完成，记录数: 9
2025-08-04 10:28:32,705 - WARNING - 表 ads_shop_page_analysis_pc 中发现重复列: dt，已跳过
2025-08-04 10:28:32,793 - INFO - 表 ads_shop_page_analysis_pc 结构同步成功
2025-08-04 10:28:33,009 - INFO - 表 ads_shop_page_analysis_pc 数据同步完成，记录数: 5
2025-08-04 10:28:33,100 - WARNING - 表 ads_shop_page_analysis_wireless 中发现重复列: dt，已跳过
2025-08-04 10:28:33,175 - INFO - 表 ads_shop_page_analysis_wireless 结构同步成功
2025-08-04 10:28:33,379 - INFO - 表 ads_shop_page_analysis_wireless 数据同步完成，记录数: 5
2025-08-04 10:28:33,465 - WARNING - 表 ads_wireless_entry_indicator 中发现重复列: dt，已跳过
2025-08-04 10:28:33,549 - INFO - 表 ads_wireless_entry_indicator 结构同步成功
2025-08-04 10:28:33,734 - INFO - 表 ads_wireless_entry_indicator 数据同步完成，记录数: 9
2025-08-04 10:28:33,836 - WARNING - 表 dwd_instore_path_pc 中发现重复列: dt，已跳过
2025-08-04 10:28:33,917 - INFO - 表 dwd_instore_path_pc 结构同步成功
2025-08-04 10:28:58,400 - INFO - 表 dwd_instore_path_pc 数据同步完成，记录数: 38501
2025-08-04 10:28:58,491 - WARNING - 表 dwd_instore_path_wireless 中发现重复列: dt，已跳过
2025-08-04 10:28:58,588 - INFO - 表 dwd_instore_path_wireless 结构同步成功
2025-08-04 10:29:02,640 - INFO - Closing down clientserver connection
2025-08-04 15:37:53,220 - INFO - 发现 15 张待同步表
2025-08-04 15:37:53,438 - WARNING - 表 ads_comprehensive_page_analysis 中发现重复列: dt，已跳过
2025-08-04 15:37:53,579 - INFO - 表 ads_comprehensive_page_analysis 结构同步成功
2025-08-04 15:37:57,739 - INFO - 表 ads_comprehensive_page_analysis 数据同步完成，记录数: 2904
2025-08-04 15:37:57,835 - WARNING - 表 ads_page_click_dashboard 中发现重复列: dt，已跳过
2025-08-04 15:37:57,924 - INFO - 表 ads_page_click_dashboard 结构同步成功
2025-08-04 15:37:59,909 - INFO - 表 ads_page_click_dashboard 数据同步完成，记录数: 2904
2025-08-04 15:37:59,994 - WARNING - 表 ads_page_guidance_dashboard 中发现重复列: dt，已跳过
2025-08-04 15:38:00,064 - INFO - 表 ads_page_guidance_dashboard 结构同步成功
2025-08-04 15:38:01,974 - INFO - 表 ads_page_guidance_dashboard 数据同步完成，记录数: 2904
2025-08-04 15:38:02,071 - WARNING - 表 ads_page_traffic_dashboard 中发现重复列: dt，已跳过
2025-08-04 15:38:02,158 - INFO - 表 ads_page_traffic_dashboard 结构同步成功
2025-08-04 15:38:04,477 - INFO - 表 ads_page_traffic_dashboard 数据同步完成，记录数: 2904
2025-08-04 15:38:04,577 - WARNING - 表 dim_product 中发现重复列: dt，已跳过
2025-08-04 15:38:04,667 - ERROR - 同步表 dim_product 结构失败: (1067, "Invalid default value for 'update_time'")
2025-08-04 15:38:04,667 - ERROR - 表 dim_product 同步失败，跳过继续处理其他表
2025-08-04 15:38:04,757 - WARNING - 表 dim_store 中发现重复列: dt，已跳过
2025-08-04 15:38:04,837 - ERROR - 同步表 dim_store 结构失败: (1067, "Invalid default value for 'update_time'")
2025-08-04 15:38:04,837 - ERROR - 表 dim_store 同步失败，跳过继续处理其他表
2025-08-04 15:38:04,918 - WARNING - 表 dim_user 中发现重复列: dt，已跳过
2025-08-04 15:38:04,992 - ERROR - 同步表 dim_user 结构失败: (1067, "Invalid default value for 'last_login_time'")
2025-08-04 15:38:04,992 - ERROR - 表 dim_user 同步失败，跳过继续处理其他表
2025-08-04 15:38:05,071 - WARNING - 表 dwd_user_behavior_log 中发现重复列: dt，已跳过
2025-08-04 15:38:05,149 - INFO - 表 dwd_user_behavior_log 结构同步成功
2025-08-04 15:38:07,158 - INFO - 表 dwd_user_behavior_log 数据同步完成，记录数: 3000
2025-08-04 15:38:07,234 - WARNING - 表 dws_page_click_distribution_d 中发现重复列: dt，已跳过
2025-08-04 15:38:07,340 - INFO - 表 dws_page_click_distribution_d 结构同步成功
2025-08-04 15:38:09,506 - INFO - 表 dws_page_click_distribution_d 数据同步完成，记录数: 2904
2025-08-04 15:38:09,594 - WARNING - 表 dws_page_guidance_effect_d 中发现重复列: dt，已跳过
2025-08-04 15:38:09,703 - INFO - 表 dws_page_guidance_effect_d 结构同步成功
2025-08-04 15:38:11,642 - INFO - 表 dws_page_guidance_effect_d 数据同步完成，记录数: 2904
2025-08-04 15:38:11,742 - WARNING - 表 dws_page_traffic_analysis_d 中发现重复列: dt，已跳过
2025-08-04 15:38:11,818 - INFO - 表 dws_page_traffic_analysis_d 结构同步成功
2025-08-04 15:38:13,750 - INFO - 表 dws_page_traffic_analysis_d 数据同步完成，记录数: 2904
2025-08-04 15:38:13,832 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-04 15:38:13,926 - ERROR - 同步表 ods_product_info 结构失败: (1067, "Invalid default value for 'update_time'")
2025-08-04 15:38:13,926 - ERROR - 表 ods_product_info 同步失败，跳过继续处理其他表
2025-08-04 15:38:14,018 - WARNING - 表 ods_store_info 中发现重复列: dt，已跳过
2025-08-04 15:38:14,096 - ERROR - 同步表 ods_store_info 结构失败: (1067, "Invalid default value for 'update_time'")
2025-08-04 15:38:14,096 - ERROR - 表 ods_store_info 同步失败，跳过继续处理其他表
2025-08-04 15:38:14,174 - WARNING - 表 ods_user_behavior_log 中发现重复列: dt，已跳过
2025-08-04 15:38:14,248 - ERROR - 同步表 ods_user_behavior_log 结构失败: (1067, "Invalid default value for 'create_time'")
2025-08-04 15:38:14,249 - ERROR - 表 ods_user_behavior_log 同步失败，跳过继续处理其他表
2025-08-04 15:38:14,322 - WARNING - 表 ods_user_info 中发现重复列: dt，已跳过
2025-08-04 15:38:14,405 - ERROR - 同步表 ods_user_info 结构失败: (1067, "Invalid default value for 'last_login_time'")
2025-08-04 15:38:14,405 - ERROR - 表 ods_user_info 同步失败，跳过继续处理其他表
2025-08-04 15:38:14,835 - INFO - Closing down clientserver connection
2025-08-07 15:11:37,503 - INFO - 发现 16 张待同步表
2025-08-07 15:11:37,889 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 15:11:40,889 - INFO - 表 ads_inventory_warning 数据同步完成，记录数: 361
2025-08-07 15:11:41,009 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 15:11:41,104 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 15:11:42,877 - INFO - 表 ads_product_comprehensive_ranking 数据同步完成，记录数: 1135
2025-08-07 15:11:42,970 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 15:11:43,061 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 15:11:43,423 - ERROR - 同步表 ads_traffic_effect_analysis 数据失败: An error occurred while calling o79.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 1 times, most recent failure: Lost task 3.0 in stage 2.0 (TID 19) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-3138a897-ab05-4eae-b4da-4c6a48a2f5a9.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-3138a897-ab05-4eae-b4da-4c6a48a2f5a9.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-07 15:11:43,426 - ERROR - 表 ads_traffic_effect_analysis 同步失败，跳过继续处理其他表
2025-08-07 15:11:43,629 - INFO - 表 dim_product_category 结构同步成功
2025-08-07 15:11:44,029 - INFO - 表 dim_product_category 数据同步完成，记录数: 8
2025-08-07 15:11:44,205 - INFO - 表 dim_traffic_source 结构同步成功
2025-08-07 15:11:44,353 - INFO - 表 dim_traffic_source 数据同步完成，记录数: 6
2025-08-07 15:11:44,451 - WARNING - 表 dwd_product_sale_detail 中发现重复列: dt，已跳过
2025-08-07 15:11:44,543 - INFO - 表 dwd_product_sale_detail 结构同步成功
2025-08-07 15:11:45,714 - INFO - 表 dwd_product_sale_detail 数据同步完成，记录数: 718
2025-08-07 15:11:45,800 - WARNING - 表 dwd_product_search_detail 中发现重复列: dt，已跳过
2025-08-07 15:11:45,888 - INFO - 表 dwd_product_search_detail 结构同步成功
2025-08-07 15:11:46,808 - INFO - 表 dwd_product_search_detail 数据同步完成，记录数: 1138
2025-08-07 15:11:46,894 - WARNING - 表 dwd_product_visit_detail 中发现重复列: dt，已跳过
2025-08-07 15:11:46,986 - INFO - 表 dwd_product_visit_detail 结构同步成功
2025-08-07 15:11:48,541 - INFO - 表 dwd_product_visit_detail 数据同步完成，记录数: 2241
2025-08-07 15:11:48,647 - WARNING - 表 dws_product_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 15:11:48,750 - INFO - 表 dws_product_sale_summary 结构同步成功
2025-08-07 15:11:49,235 - INFO - 表 dws_product_sale_summary 数据同步完成，记录数: 327
2025-08-07 15:11:49,351 - WARNING - 表 dws_product_visit_summary 中发现重复列: stat_period，已跳过
2025-08-07 15:11:49,446 - INFO - 表 dws_product_visit_summary 结构同步成功
2025-08-07 15:11:49,777 - ERROR - 同步表 dws_product_visit_summary 数据失败: An error occurred while calling o172.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 9.0 failed 1 times, most recent failure: Lost task 3.0 in stage 9.0 (TID 52) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-352bd312-494e-46fc-8f71-e934e2226598.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-352bd312-494e-46fc-8f71-e934e2226598.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-07 15:11:49,779 - ERROR - 表 dws_product_visit_summary 同步失败，跳过继续处理其他表
2025-08-07 15:11:49,903 - WARNING - 表 dws_search_word_summary 中发现重复列: stat_period，已跳过
2025-08-07 15:11:50,002 - INFO - 表 dws_search_word_summary 结构同步成功
2025-08-07 15:11:50,232 - ERROR - 同步表 dws_search_word_summary 数据失败: (1241, 'Operand should contain 1 column(s)')
2025-08-07 15:11:50,232 - ERROR - 表 dws_search_word_summary 同步失败，跳过继续处理其他表
2025-08-07 15:11:50,356 - WARNING - 表 dws_sku_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 15:11:50,453 - INFO - 表 dws_sku_sale_summary 结构同步成功
2025-08-07 15:11:51,075 - INFO - 表 dws_sku_sale_summary 数据同步完成，记录数: 503
2025-08-07 15:11:51,197 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-07 15:11:51,349 - INFO - 表 ods_product_info 结构同步成功
2025-08-07 15:11:53,995 - INFO - 表 ods_product_info 数据同步完成，记录数: 3100
2025-08-07 15:11:54,101 - WARNING - 表 ods_product_sale_info 中发现重复列: dt，已跳过
2025-08-07 15:11:54,188 - INFO - 表 ods_product_sale_info 结构同步成功
2025-08-07 15:11:56,517 - INFO - 表 ods_product_sale_info 数据同步完成，记录数: 3000
2025-08-07 15:11:56,599 - WARNING - 表 ods_search_log 中发现重复列: dt，已跳过
2025-08-07 15:11:56,703 - INFO - 表 ods_search_log 结构同步成功
2025-08-07 15:11:59,993 - INFO - 表 ods_search_log 数据同步完成，记录数: 5000
2025-08-07 15:12:00,078 - WARNING - 表 ods_shop_visit_log 中发现重复列: dt，已跳过
2025-08-07 15:12:00,165 - INFO - 表 ods_shop_visit_log 结构同步成功
2025-08-07 15:12:06,572 - INFO - 表 ods_shop_visit_log 数据同步完成，记录数: 10000
2025-08-07 15:12:06,888 - INFO - Closing down clientserver connection
2025-08-07 15:13:02,943 - INFO - 发现 16 张待同步表
2025-08-07 15:13:03,284 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 15:13:05,792 - INFO - 表 ads_inventory_warning 数据同步完成，记录数: 361
2025-08-07 15:13:05,914 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 15:13:06,016 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 15:13:08,253 - INFO - 表 ads_product_comprehensive_ranking 数据同步完成，记录数: 1135
2025-08-07 15:13:08,372 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 15:13:08,467 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 15:13:08,809 - ERROR - 同步表 ads_traffic_effect_analysis 数据失败: An error occurred while calling o79.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 18) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-6a264a8e-850d-49ee-96a8-5b7096946116.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-6a264a8e-850d-49ee-96a8-5b7096946116.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-07 15:13:08,816 - ERROR - 表 ads_traffic_effect_analysis 同步失败，跳过继续处理其他表
2025-08-07 15:13:09,023 - INFO - 表 dim_product_category 结构同步成功
2025-08-07 15:13:09,336 - INFO - 表 dim_product_category 数据同步完成，记录数: 8
2025-08-07 15:13:09,520 - INFO - 表 dim_traffic_source 结构同步成功
2025-08-07 15:13:09,662 - INFO - 表 dim_traffic_source 数据同步完成，记录数: 6
2025-08-07 15:13:09,762 - WARNING - 表 dwd_product_sale_detail 中发现重复列: dt，已跳过
2025-08-07 15:13:09,861 - INFO - 表 dwd_product_sale_detail 结构同步成功
2025-08-07 15:13:10,578 - INFO - 表 dwd_product_sale_detail 数据同步完成，记录数: 718
2025-08-07 15:13:10,657 - WARNING - 表 dwd_product_search_detail 中发现重复列: dt，已跳过
2025-08-07 15:13:10,738 - INFO - 表 dwd_product_search_detail 结构同步成功
2025-08-07 15:13:11,615 - INFO - 表 dwd_product_search_detail 数据同步完成，记录数: 1138
2025-08-07 15:13:11,702 - WARNING - 表 dwd_product_visit_detail 中发现重复列: dt，已跳过
2025-08-07 15:13:11,771 - INFO - 表 dwd_product_visit_detail 结构同步成功
2025-08-07 15:13:13,319 - INFO - 表 dwd_product_visit_detail 数据同步完成，记录数: 2241
2025-08-07 15:13:13,399 - WARNING - 表 dws_product_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 15:13:13,494 - INFO - 表 dws_product_sale_summary 结构同步成功
2025-08-07 15:13:13,917 - INFO - 表 dws_product_sale_summary 数据同步完成，记录数: 327
2025-08-07 15:13:13,999 - WARNING - 表 dws_product_visit_summary 中发现重复列: stat_period，已跳过
2025-08-07 15:13:14,063 - INFO - 表 dws_product_visit_summary 结构同步成功
2025-08-07 15:13:14,355 - ERROR - 同步表 dws_product_visit_summary 数据失败: An error occurred while calling o172.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 9.0 failed 1 times, most recent failure: Lost task 4.0 in stage 9.0 (TID 53) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-bf0f9e93-acad-4d7f-9549-0721355e9bba.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-bf0f9e93-acad-4d7f-9549-0721355e9bba.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-07 15:13:14,361 - ERROR - 表 dws_product_visit_summary 同步失败，跳过继续处理其他表
2025-08-07 15:13:14,442 - WARNING - 表 dws_search_word_summary 中发现重复列: stat_period，已跳过
2025-08-07 15:13:14,517 - INFO - 表 dws_search_word_summary 结构同步成功
2025-08-07 15:13:14,728 - ERROR - 同步表 dws_search_word_summary 数据失败: (1241, 'Operand should contain 1 column(s)')
2025-08-07 15:13:14,728 - ERROR - 表 dws_search_word_summary 同步失败，跳过继续处理其他表
2025-08-07 15:13:14,807 - WARNING - 表 dws_sku_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 15:13:14,886 - INFO - 表 dws_sku_sale_summary 结构同步成功
2025-08-07 15:13:15,410 - INFO - 表 dws_sku_sale_summary 数据同步完成，记录数: 503
2025-08-07 15:13:15,491 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-07 15:13:15,578 - INFO - 表 ods_product_info 结构同步成功
2025-08-07 15:13:16,721 - INFO - Closing down clientserver connection
2025-08-07 16:13:38,750 - INFO - 发现 16 张待同步表
2025-08-07 16:13:39,113 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 16:13:39,592 - ERROR - 同步表 ads_inventory_warning 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`stat_date`, `product_name`, `product_id`, `sku_id`, `current_stock`].; line 1 pos 15;
'Project [product_id#41, product_name#42, sku_id#43, sku_info#44, current_stock#45, recent_avg_sales#46L, stock_coverage_days#47L, warning_level#48, warning_message#49, stat_date#50, cast('add_cart_rate as double) AS add_cart_rate#40]
+- SubqueryAlias spark_catalog.gd02.ads_inventory_warning
   +- Relation spark_catalog.gd02.ads_inventory_warning[product_id#41,product_name#42,sku_id#43,sku_info#44,current_stock#45,recent_avg_sales#46L,stock_coverage_days#47L,warning_level#48,warning_message#49,stat_date#50] parquet

2025-08-07 16:13:39,592 - ERROR - 表 ads_inventory_warning 同步失败，跳过继续处理其他表
2025-08-07 16:13:39,705 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 16:13:39,778 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 16:13:39,855 - ERROR - 同步表 ads_product_comprehensive_ranking 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`stat_date`, `overall_rank`, `product_name`, `avg_price`, `category_name`].; line 1 pos 15;
'Project [product_id#91, product_name#92, category_id#93, category_name#94, total_sales#95, total_volume#96L, avg_price#97, visitor_count#98L, conversion_rate#99, search_count#100L, comprehensive_score#101, overall_rank#102, category_rank#103, stat_date#104, stat_period#105, cast('add_cart_rate as double) AS add_cart_rate#90]
+- SubqueryAlias spark_catalog.gd02.ads_product_comprehensive_ranking
   +- Relation spark_catalog.gd02.ads_product_comprehensive_ranking[product_id#91,product_name#92,category_id#93,category_name#94,total_sales#95,total_volume#96L,avg_price#97,visitor_count#98L,conversion_rate#99,search_count#100L,comprehensive_score#101,overall_rank#102,category_rank#103,stat_date#104,stat_period#105] parquet

2025-08-07 16:13:39,855 - ERROR - 表 ads_product_comprehensive_ranking 同步失败，跳过继续处理其他表
2025-08-07 16:13:39,945 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:13:40,022 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 16:13:42,556 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 30
2025-08-07 16:13:42,731 - INFO - 表 dim_product_category 结构同步成功
2025-08-07 16:13:42,795 - ERROR - 同步表 dim_product_category 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`category_name`, `category_id`, `parent_category_id`].; line 1 pos 15;
'Project [category_id#226, category_name#227, parent_category_id#228, cast('add_cart_rate as double) AS add_cart_rate#225]
+- SubqueryAlias spark_catalog.gd02.dim_product_category
   +- Relation spark_catalog.gd02.dim_product_category[category_id#226,category_name#227,parent_category_id#228] orc

2025-08-07 16:13:42,795 - ERROR - 表 dim_product_category 同步失败，跳过继续处理其他表
2025-08-07 16:13:43,058 - INFO - 表 dim_traffic_source 结构同步成功
2025-08-07 16:13:43,120 - ERROR - 同步表 dim_traffic_source 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`source_name`, `source_type`].; line 1 pos 15;
'Project [source_name#249, source_type#250, cast('add_cart_rate as double) AS add_cart_rate#248]
+- SubqueryAlias spark_catalog.gd02.dim_traffic_source
   +- Relation spark_catalog.gd02.dim_traffic_source[source_name#249,source_type#250] orc

2025-08-07 16:13:43,120 - ERROR - 表 dim_traffic_source 同步失败，跳过继续处理其他表
2025-08-07 16:13:43,208 - WARNING - 表 dwd_product_sale_detail 中发现重复列: dt，已跳过
2025-08-07 16:13:43,292 - INFO - 表 dwd_product_sale_detail 结构同步成功
2025-08-07 16:13:43,342 - ERROR - 同步表 dwd_product_sale_detail 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`category_name`, `dt`, `order_id`, `pay_amount`, `pay_num`].; line 1 pos 15;
'Project [order_id#278, product_id#279, sku_id#280, user_id#281, pay_amount#282, pay_num#283, pay_time#284, category_id#285, category_name#286, dt#287, cast('add_cart_rate as double) AS add_cart_rate#277]
+- SubqueryAlias spark_catalog.gd02.dwd_product_sale_detail
   +- Relation spark_catalog.gd02.dwd_product_sale_detail[order_id#278,product_id#279,sku_id#280,user_id#281,pay_amount#282,pay_num#283,pay_time#284,category_id#285,category_name#286,dt#287] parquet

2025-08-07 16:13:43,343 - ERROR - 表 dwd_product_sale_detail 同步失败，跳过继续处理其他表
2025-08-07 16:13:43,422 - WARNING - 表 dwd_product_search_detail 中发现重复列: dt，已跳过
2025-08-07 16:13:43,508 - INFO - 表 dwd_product_search_detail 结构同步成功
2025-08-07 16:13:43,596 - ERROR - 同步表 dwd_product_search_detail 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`category_name`, `dt`, `product_id`, `search_time`, `user_id`].; line 1 pos 15;
'Project [user_id#320, search_word#321, product_id#322, search_time#323, category_id#324, category_name#325, dt#326, cast('add_cart_rate as double) AS add_cart_rate#319]
+- SubqueryAlias spark_catalog.gd02.dwd_product_search_detail
   +- Relation spark_catalog.gd02.dwd_product_search_detail[user_id#320,search_word#321,product_id#322,search_time#323,category_id#324,category_name#325,dt#326] parquet

2025-08-07 16:13:43,596 - ERROR - 表 dwd_product_search_detail 同步失败，跳过继续处理其他表
2025-08-07 16:13:43,677 - WARNING - 表 dwd_product_visit_detail 中发现重复列: dt，已跳过
2025-08-07 16:13:43,760 - INFO - 表 dwd_product_visit_detail 结构同步成功
2025-08-07 16:13:43,808 - ERROR - 同步表 dwd_product_visit_detail 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`dt`, `product_id`, `source_name`, `stay_time`, `user_id`].; line 1 pos 15;
'Project [user_id#358, product_id#359, category_id#360, visit_time#361, source_id#362, source_name#363, source_type#364, stay_time#365, dt#366, cast('add_cart_rate as double) AS add_cart_rate#357]
+- SubqueryAlias spark_catalog.gd02.dwd_product_visit_detail
   +- Relation spark_catalog.gd02.dwd_product_visit_detail[user_id#358,product_id#359,category_id#360,visit_time#361,source_id#362,source_name#363,source_type#364,stay_time#365,dt#366] parquet

2025-08-07 16:13:43,809 - ERROR - 表 dwd_product_visit_detail 同步失败，跳过继续处理其他表
2025-08-07 16:13:43,888 - WARNING - 表 dws_product_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:13:43,960 - INFO - 表 dws_product_sale_summary 结构同步成功
2025-08-07 16:13:44,014 - ERROR - 同步表 dws_product_sale_summary 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`stat_date`, `product_name`, `avg_price`, `category_name`, `product_id`].; line 1 pos 15;
'Project [product_id#401, product_name#402, category_id#403, category_name#404, total_sales_amount#405, total_sales_num#406L, total_order_num#407L, avg_price#408, stat_date#409, stat_period#410, cast('add_cart_rate as double) AS add_cart_rate#400]
+- SubqueryAlias spark_catalog.gd02.dws_product_sale_summary
   +- Relation spark_catalog.gd02.dws_product_sale_summary[product_id#401,product_name#402,category_id#403,category_name#404,total_sales_amount#405,total_sales_num#406L,total_order_num#407L,avg_price#408,stat_date#409,stat_period#410] parquet

2025-08-07 16:13:44,015 - ERROR - 表 dws_product_sale_summary 同步失败，跳过继续处理其他表
2025-08-07 16:13:44,098 - WARNING - 表 dws_product_visit_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:13:44,164 - INFO - 表 dws_product_visit_summary 结构同步成功
2025-08-07 16:13:44,217 - ERROR - 同步表 dws_product_visit_summary 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`stat_date`, `product_id`, `category_id`, `stat_period`, `pay_conversion_rate`].; line 1 pos 15;
'Project [product_id#444, category_id#445, total_visitor_num#446L, total_visit_num#447L, source_visitor_map#448, pay_conversion_rate#449, stat_date#450, stat_period#451, cast('add_cart_rate as double) AS add_cart_rate#443]
+- SubqueryAlias spark_catalog.gd02.dws_product_visit_summary
   +- Relation spark_catalog.gd02.dws_product_visit_summary[product_id#444,category_id#445,total_visitor_num#446L,total_visit_num#447L,source_visitor_map#448,pay_conversion_rate#449,stat_date#450,stat_period#451] parquet

2025-08-07 16:13:44,217 - ERROR - 表 dws_product_visit_summary 同步失败，跳过继续处理其他表
2025-08-07 16:13:44,297 - WARNING - 表 dws_search_word_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:13:44,364 - INFO - 表 dws_search_word_summary 结构同步成功
2025-08-07 16:13:44,415 - ERROR - 同步表 dws_search_word_summary 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`stat_date`, `search_word`, `stat_period`, `total_search_num`, `related_product_ids`].; line 1 pos 15;
'Project [search_word#481, total_search_num#482L, total_visitor_num#483L, related_product_ids#484, stat_date#485, stat_period#486, cast('add_cart_rate as double) AS add_cart_rate#480]
+- SubqueryAlias spark_catalog.gd02.dws_search_word_summary
   +- Relation spark_catalog.gd02.dws_search_word_summary[search_word#481,total_search_num#482L,total_visitor_num#483L,related_product_ids#484,stat_date#485,stat_period#486] parquet

2025-08-07 16:13:44,415 - ERROR - 表 dws_search_word_summary 同步失败，跳过继续处理其他表
2025-08-07 16:13:44,493 - WARNING - 表 dws_sku_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:13:44,594 - INFO - 表 dws_sku_sale_summary 结构同步成功
2025-08-07 16:13:44,645 - ERROR - 同步表 dws_sku_sale_summary 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`stat_date`, `pay_num_ratio`, `product_id`, `sku_id`, `stat_period`].; line 1 pos 15;
'Project [sku_id#516, product_id#517, total_pay_num#518L, pay_num_ratio#519, current_stock#520, stock_sale_days#521L, stat_date#522, stat_period#523, cast('add_cart_rate as double) AS add_cart_rate#515]
+- SubqueryAlias spark_catalog.gd02.dws_sku_sale_summary
   +- Relation spark_catalog.gd02.dws_sku_sale_summary[sku_id#516,product_id#517,total_pay_num#518L,pay_num_ratio#519,current_stock#520,stock_sale_days#521L,stat_date#522,stat_period#523] parquet

2025-08-07 16:13:44,645 - ERROR - 表 dws_sku_sale_summary 同步失败，跳过继续处理其他表
2025-08-07 16:13:44,887 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-07 16:13:45,347 - INFO - 表 ods_product_info 结构同步成功
2025-08-07 16:13:45,397 - ERROR - 同步表 ods_product_info 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`product_name`, `dt`, `product_id`, `stock_num`, `category_id`].; line 1 pos 15;
'Project [product_id#554, product_name#555, category_id#556, price#557, stock_num#558, price_strength#559, dt#560, cast('add_cart_rate as double) AS add_cart_rate#553]
+- SubqueryAlias spark_catalog.gd02.ods_product_info
   +- Relation spark_catalog.gd02.ods_product_info[product_id#554,product_name#555,category_id#556,price#557,stock_num#558,price_strength#559,dt#560] orc

2025-08-07 16:13:45,397 - ERROR - 表 ods_product_info 同步失败，跳过继续处理其他表
2025-08-07 16:13:45,475 - WARNING - 表 ods_product_sale_info 中发现重复列: dt，已跳过
2025-08-07 16:13:45,576 - INFO - 表 ods_product_sale_info 结构同步成功
2025-08-07 16:13:45,624 - ERROR - 同步表 ods_product_sale_info 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `add_cart_rate` cannot be resolved. Did you mean one of the following? [`dt`, `order_id`, `pay_amount`, `pay_num`, `pay_time`].; line 1 pos 15;
'Project [order_id#592, product_id#593, sku_id#594, user_id#595, pay_amount#596, pay_num#597, pay_time#598, category_id#599, dt#600, cast('add_cart_rate as double) AS add_cart_rate#591]
+- SubqueryAlias spark_catalog.gd02.ods_product_sale_info
   +- Relation spark_catalog.gd02.ods_product_sale_info[order_id#592,product_id#593,sku_id#594,user_id#595,pay_amount#596,pay_num#597,pay_time#598,category_id#599,dt#600] orc

2025-08-07 16:13:45,625 - ERROR - 表 ods_product_sale_info 同步失败，跳过继续处理其他表
2025-08-07 16:13:45,707 - WARNING - 表 ods_search_log 中发现重复列: dt，已跳过
2025-08-07 16:13:45,777 - INFO - Error while receiving.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=380>
2025-08-07 16:13:45,798 - INFO - Closing down clientserver connection
2025-08-07 16:13:45,799 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=380>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-08-07 16:13:45,801 - INFO - Error while receiving.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o27.sc
2025-08-07 16:13:45,827 - INFO - Closing down clientserver connection
2025-08-07 16:13:45,827 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o27.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-08-07 16:13:45,829 - INFO - Closing down clientserver connection
2025-08-07 16:13:45,829 - ERROR - 表 ods_search_log 同步失败，跳过继续处理其他表
2025-08-07 16:13:46,306 - INFO - Error while receiving.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-08-07 16:13:46,307 - INFO - Error while receiving.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-08-07 16:13:46,308 - INFO - Closing down clientserver connection
2025-08-07 16:13:46,309 - INFO - Closing down clientserver connection
2025-08-07 16:13:46,309 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-08-07 16:13:46,309 - ERROR - 表 ods_shop_visit_log 同步失败，跳过继续处理其他表
2025-08-07 16:13:46,310 - INFO - Closing down clientserver connection
2025-08-07 16:13:46,310 - INFO - Closing down clientserver connection
2025-08-07 16:13:48,353 - INFO - Closing down clientserver connection
2025-08-07 16:13:48,353 - INFO - Closing down clientserver connection
2025-08-07 16:13:48,355 - INFO - Closing down clientserver connection
2025-08-07 16:13:48,378 - INFO - Closing down clientserver connection
2025-08-07 16:29:12,523 - INFO - 发现 3 张待同步表: ['ads_traffic_effect_analysis', 'ads_product_comprehensive_ranking', 'ads_inventory_warning']
2025-08-07 16:29:12,523 - INFO - 开始同步表 ads_traffic_effect_analysis
2025-08-07 16:29:12,751 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:29:12,852 - ERROR - 表 ads_traffic_effect_analysis 同步失败: 'HiveToMySQLSync' object has no attribute 'generate_mysql_ddl'，跳过继续处理其他表
2025-08-07 16:29:12,853 - INFO - 开始同步表 ads_product_comprehensive_ranking
2025-08-07 16:29:12,951 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 16:29:13,029 - ERROR - 表 ads_product_comprehensive_ranking 同步失败: 'HiveToMySQLSync' object has no attribute 'generate_mysql_ddl'，跳过继续处理其他表
2025-08-07 16:29:13,029 - INFO - 开始同步表 ads_inventory_warning
2025-08-07 16:29:13,181 - ERROR - 表 ads_inventory_warning 同步失败: 'HiveToMySQLSync' object has no attribute 'generate_mysql_ddl'，跳过继续处理其他表
2025-08-07 16:29:13,634 - INFO - Closing down clientserver connection
2025-08-07 16:32:08,758 - INFO - 发现 3 张待同步表: ['ads_traffic_effect_analysis', 'ads_product_comprehensive_ranking', 'ads_inventory_warning']
2025-08-07 16:32:08,758 - INFO - 开始同步表 ads_traffic_effect_analysis
2025-08-07 16:32:09,003 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:32:09,124 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 16:32:11,518 - INFO - NumExpr defaulting to 16 threads.
2025-08-07 16:32:15,416 - ERROR - 同步表 ads_traffic_effect_analysis 数据失败: An error occurred while calling o56.getResult.
: org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:98)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-aafcaf9c-0b5d-4c17-8b51-0c7a511152b0.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.foreach(ArrowConverters.scala:77)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.to(ArrowConverters.scala:77)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.toBuffer(ArrowConverters.scala:77)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.toArray(ArrowConverters.scala:77)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4265)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2487)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 36 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:4263)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4267)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4243)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:4243)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:4242)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:140)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:142)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:137)
	at org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:114)
	at org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:108)
	at org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:69)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:69)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-aafcaf9c-0b5d-4c17-8b51-0c7a511152b0.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.foreach(ArrowConverters.scala:77)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.to(ArrowConverters.scala:77)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.toBuffer(ArrowConverters.scala:77)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.toArray(ArrowConverters.scala:77)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4265)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2487)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 36 more

2025-08-07 16:32:15,422 - ERROR - 表 ads_traffic_effect_analysis 同步失败: An error occurred while calling o56.getResult.
: org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:98)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-aafcaf9c-0b5d-4c17-8b51-0c7a511152b0.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.foreach(ArrowConverters.scala:77)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.to(ArrowConverters.scala:77)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.toBuffer(ArrowConverters.scala:77)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.toArray(ArrowConverters.scala:77)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4265)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2487)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 36 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:4263)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4267)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4243)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:4243)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:4242)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:140)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:142)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:137)
	at org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:114)
	at org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:108)
	at org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:69)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:69)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-aafcaf9c-0b5d-4c17-8b51-0c7a511152b0.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.hasNext(ArrowConverters.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.foreach(ArrowConverters.scala:77)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.to(ArrowConverters.scala:77)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.toBuffer(ArrowConverters.scala:77)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.toArray(ArrowConverters.scala:77)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4265)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2487)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 36 more
，跳过继续处理其他表
2025-08-07 16:32:15,427 - INFO - 开始同步表 ads_product_comprehensive_ranking
2025-08-07 16:32:15,537 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 16:32:15,645 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 16:32:16,168 - INFO - 开始同步表 ads_product_comprehensive_ranking 的 1135 条数据
2025-08-07 16:32:16,269 - ERROR - 同步表 ads_product_comprehensive_ranking 数据失败: (1062, "Duplicate entry '51-2025-01-04' for key 'PRIMARY'")
2025-08-07 16:32:16,269 - ERROR - 表 ads_product_comprehensive_ranking 同步失败: (1062, "Duplicate entry '51-2025-01-04' for key 'PRIMARY'")，跳过继续处理其他表
2025-08-07 16:32:16,270 - INFO - 开始同步表 ads_inventory_warning
2025-08-07 16:32:16,464 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 16:32:16,741 - INFO - 开始同步表 ads_inventory_warning 的 361 条数据
2025-08-07 16:32:16,777 - ERROR - 同步表 ads_inventory_warning 数据失败: (1062, "Duplicate entry '63-473-2025-01-04' for key 'PRIMARY'")
2025-08-07 16:32:16,778 - ERROR - 表 ads_inventory_warning 同步失败: (1062, "Duplicate entry '63-473-2025-01-04' for key 'PRIMARY'")，跳过继续处理其他表
2025-08-07 16:32:16,971 - INFO - Closing down clientserver connection
2025-08-07 16:33:34,954 - INFO - Error while receiving.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-08-07 16:33:34,957 - INFO - Closing down clientserver connection
2025-08-07 16:33:34,957 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-08-07 16:33:34,960 - INFO - Closing down clientserver connection
2025-08-07 16:33:34,974 - INFO - Closing down clientserver connection
2025-08-07 16:34:03,277 - INFO - 发现 16 张待同步表
2025-08-07 16:34:03,669 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 16:34:06,148 - INFO - 表 ads_inventory_warning 数据同步完成，记录数: 361
2025-08-07 16:34:06,247 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 16:34:06,319 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 16:34:08,338 - INFO - 表 ads_product_comprehensive_ranking 数据同步完成，记录数: 1135
2025-08-07 16:34:08,429 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:34:08,517 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 16:34:08,872 - ERROR - 同步表 ads_traffic_effect_analysis 数据失败: An error occurred while calling o79.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 18) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-6a264a8e-850d-49ee-96a8-5b7096946116.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-6a264a8e-850d-49ee-96a8-5b7096946116.c000.snappy.parquet. Column: [add_cart_rate], Expected: double, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: double
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-07 16:34:08,872 - ERROR - 表 ads_traffic_effect_analysis 同步失败，跳过继续处理其他表
2025-08-07 16:34:09,066 - INFO - 表 dim_product_category 结构同步成功
2025-08-07 16:34:09,426 - INFO - 表 dim_product_category 数据同步完成，记录数: 8
2025-08-07 16:34:09,607 - INFO - 表 dim_traffic_source 结构同步成功
2025-08-07 16:34:09,749 - INFO - 表 dim_traffic_source 数据同步完成，记录数: 6
2025-08-07 16:34:09,840 - WARNING - 表 dwd_product_sale_detail 中发现重复列: dt，已跳过
2025-08-07 16:34:09,934 - INFO - 表 dwd_product_sale_detail 结构同步成功
2025-08-07 16:34:10,624 - INFO - 表 dwd_product_sale_detail 数据同步完成，记录数: 718
2025-08-07 16:34:10,717 - WARNING - 表 dwd_product_search_detail 中发现重复列: dt，已跳过
2025-08-07 16:34:10,804 - INFO - 表 dwd_product_search_detail 结构同步成功
2025-08-07 16:34:11,675 - INFO - 表 dwd_product_search_detail 数据同步完成，记录数: 1138
2025-08-07 16:34:11,755 - WARNING - 表 dwd_product_visit_detail 中发现重复列: dt，已跳过
2025-08-07 16:34:11,839 - INFO - 表 dwd_product_visit_detail 结构同步成功
2025-08-07 16:34:13,408 - INFO - 表 dwd_product_visit_detail 数据同步完成，记录数: 2241
2025-08-07 16:34:13,500 - WARNING - 表 dws_product_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:34:13,618 - INFO - 表 dws_product_sale_summary 结构同步成功
2025-08-07 16:34:14,012 - INFO - 表 dws_product_sale_summary 数据同步完成，记录数: 327
2025-08-07 16:34:14,095 - WARNING - 表 dws_product_visit_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:34:14,173 - INFO - 表 dws_product_visit_summary 结构同步成功
2025-08-07 16:34:14,451 - ERROR - 同步表 dws_product_visit_summary 数据失败: An error occurred while calling o172.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 9.0 failed 1 times, most recent failure: Lost task 4.0 in stage 9.0 (TID 53) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-bf0f9e93-acad-4d7f-9549-0721355e9bba.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-bf0f9e93-acad-4d7f-9549-0721355e9bba.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-07 16:34:14,454 - ERROR - 表 dws_product_visit_summary 同步失败，跳过继续处理其他表
2025-08-07 16:34:14,534 - WARNING - 表 dws_search_word_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:34:14,637 - INFO - 表 dws_search_word_summary 结构同步成功
2025-08-07 16:34:14,850 - ERROR - 同步表 dws_search_word_summary 数据失败: (1241, 'Operand should contain 1 column(s)')
2025-08-07 16:34:14,850 - ERROR - 表 dws_search_word_summary 同步失败，跳过继续处理其他表
2025-08-07 16:34:14,935 - WARNING - 表 dws_sku_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:34:15,022 - INFO - 表 dws_sku_sale_summary 结构同步成功
2025-08-07 16:34:15,500 - INFO - 表 dws_sku_sale_summary 数据同步完成，记录数: 503
2025-08-07 16:34:15,596 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-07 16:34:15,690 - INFO - 表 ods_product_info 结构同步成功
2025-08-07 16:34:17,949 - INFO - 表 ods_product_info 数据同步完成，记录数: 3100
2025-08-07 16:34:18,040 - WARNING - 表 ods_product_sale_info 中发现重复列: dt，已跳过
2025-08-07 16:34:18,138 - INFO - 表 ods_product_sale_info 结构同步成功
2025-08-07 16:34:20,292 - INFO - 表 ods_product_sale_info 数据同步完成，记录数: 3000
2025-08-07 16:34:20,376 - WARNING - 表 ods_search_log 中发现重复列: dt，已跳过
2025-08-07 16:34:20,442 - INFO - 表 ods_search_log 结构同步成功
2025-08-07 16:34:23,757 - INFO - 表 ods_search_log 数据同步完成，记录数: 5000
2025-08-07 16:34:23,849 - WARNING - 表 ods_shop_visit_log 中发现重复列: dt，已跳过
2025-08-07 16:34:23,928 - INFO - 表 ods_shop_visit_log 结构同步成功
2025-08-07 16:34:30,624 - INFO - 表 ods_shop_visit_log 数据同步完成，记录数: 10000
2025-08-07 16:34:30,855 - INFO - Closing down clientserver connection
2025-08-07 16:47:01,975 - INFO - 发现 16 张待同步表
2025-08-07 16:47:02,319 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 16:47:04,616 - INFO - 表 ads_inventory_warning 数据同步完成，记录数: 361
2025-08-07 16:47:04,721 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 16:47:04,814 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 16:47:06,921 - INFO - 表 ads_product_comprehensive_ranking 数据同步完成，记录数: 1135
2025-08-07 16:47:07,027 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:47:07,123 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 16:47:07,540 - ERROR - 同步表 ads_traffic_effect_analysis 数据失败: An error occurred while calling o87.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 2.0 failed 1 times, most recent failure: Lost task 4.0 in stage 2.0 (TID 20) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-aafcaf9c-0b5d-4c17-8b51-0c7a511152b0.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-aafcaf9c-0b5d-4c17-8b51-0c7a511152b0.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 19 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableDouble cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setInt(SpecificInternalRow.scala:254)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setInt(ParquetRowConverter.scala:183)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addInt(ParquetRowConverter.scala:93)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$3.writeValue(ColumnReaderBase.java:297)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-aafcaf9c-0b5d-4c17-8b51-0c7a511152b0.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-aafcaf9c-0b5d-4c17-8b51-0c7a511152b0.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 19 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableDouble cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setInt(SpecificInternalRow.scala:254)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setInt(ParquetRowConverter.scala:183)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addInt(ParquetRowConverter.scala:93)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$3.writeValue(ColumnReaderBase.java:297)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 24 more

2025-08-07 16:47:07,541 - ERROR - 表 ads_traffic_effect_analysis 同步失败，跳过继续处理其他表
2025-08-07 16:47:07,741 - INFO - 表 dim_product_category 结构同步成功
2025-08-07 16:47:08,082 - INFO - 表 dim_product_category 数据同步完成，记录数: 8
2025-08-07 16:47:08,241 - INFO - 表 dim_traffic_source 结构同步成功
2025-08-07 16:47:08,440 - INFO - 表 dim_traffic_source 数据同步完成，记录数: 6
2025-08-07 16:47:08,522 - WARNING - 表 dwd_product_sale_detail 中发现重复列: dt，已跳过
2025-08-07 16:47:08,622 - INFO - 表 dwd_product_sale_detail 结构同步成功
2025-08-07 16:47:09,261 - INFO - 表 dwd_product_sale_detail 数据同步完成，记录数: 718
2025-08-07 16:47:09,338 - WARNING - 表 dwd_product_search_detail 中发现重复列: dt，已跳过
2025-08-07 16:47:09,441 - INFO - 表 dwd_product_search_detail 结构同步成功
2025-08-07 16:47:10,352 - INFO - 表 dwd_product_search_detail 数据同步完成，记录数: 1138
2025-08-07 16:47:10,447 - WARNING - 表 dwd_product_visit_detail 中发现重复列: dt，已跳过
2025-08-07 16:47:10,510 - INFO - 表 dwd_product_visit_detail 结构同步成功
2025-08-07 16:47:12,082 - INFO - 表 dwd_product_visit_detail 数据同步完成，记录数: 2241
2025-08-07 16:47:12,171 - WARNING - 表 dws_product_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:47:12,272 - INFO - 表 dws_product_sale_summary 结构同步成功
2025-08-07 16:47:12,641 - INFO - 表 dws_product_sale_summary 数据同步完成，记录数: 327
2025-08-07 16:47:12,731 - WARNING - 表 dws_product_visit_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:47:12,799 - INFO - 表 dws_product_visit_summary 结构同步成功
2025-08-07 16:47:13,022 - ERROR - 同步表 dws_product_visit_summary 数据失败: An error occurred while calling o181.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 9.0 failed 1 times, most recent failure: Lost task 3.0 in stage 9.0 (TID 52) (10.163.47.55 executor driver): java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:103)
	at org.apache.spark.sql.catalyst.util.GenericArrayData.getInt(GenericArrayData.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$9(ParquetFileFormat.scala:335)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.next(RecordReaderIterator.scala:62)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:103)
	at org.apache.spark.sql.catalyst.util.GenericArrayData.getInt(GenericArrayData.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$9(ParquetFileFormat.scala:335)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.next(RecordReaderIterator.scala:62)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2025-08-07 16:47:13,023 - ERROR - 表 dws_product_visit_summary 同步失败，跳过继续处理其他表
2025-08-07 16:47:13,111 - WARNING - 表 dws_search_word_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:47:13,216 - INFO - 表 dws_search_word_summary 结构同步成功
2025-08-07 16:47:13,427 - ERROR - 同步表 dws_search_word_summary 数据失败: (1241, 'Operand should contain 1 column(s)')
2025-08-07 16:47:13,427 - ERROR - 表 dws_search_word_summary 同步失败，跳过继续处理其他表
2025-08-07 16:47:13,509 - WARNING - 表 dws_sku_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:47:13,604 - INFO - 表 dws_sku_sale_summary 结构同步成功
2025-08-07 16:47:14,072 - INFO - 表 dws_sku_sale_summary 数据同步完成，记录数: 503
2025-08-07 16:47:14,161 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-07 16:47:14,268 - INFO - 表 ods_product_info 结构同步成功
2025-08-07 16:47:16,509 - INFO - 表 ods_product_info 数据同步完成，记录数: 3100
2025-08-07 16:47:16,594 - WARNING - 表 ods_product_sale_info 中发现重复列: dt，已跳过
2025-08-07 16:47:16,681 - INFO - 表 ods_product_sale_info 结构同步成功
2025-08-07 16:47:18,883 - INFO - 表 ods_product_sale_info 数据同步完成，记录数: 3000
2025-08-07 16:47:18,958 - WARNING - 表 ods_search_log 中发现重复列: dt，已跳过
2025-08-07 16:47:19,024 - INFO - 表 ods_search_log 结构同步成功
2025-08-07 16:47:22,472 - INFO - 表 ods_search_log 数据同步完成，记录数: 5000
2025-08-07 16:47:22,546 - WARNING - 表 ods_shop_visit_log 中发现重复列: dt，已跳过
2025-08-07 16:47:22,614 - INFO - 表 ods_shop_visit_log 结构同步成功
2025-08-07 16:47:28,938 - INFO - 表 ods_shop_visit_log 数据同步完成，记录数: 10000
2025-08-07 16:47:29,586 - INFO - Closing down clientserver connection
2025-08-07 16:48:45,857 - INFO - 发现 16 张待同步表
2025-08-07 16:48:46,175 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 16:48:48,771 - INFO - 表 ads_inventory_warning 数据同步完成，记录数: 361
2025-08-07 16:48:48,872 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 16:48:48,964 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 16:48:50,821 - INFO - 表 ads_product_comprehensive_ranking 数据同步完成，记录数: 1135
2025-08-07 16:48:50,917 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:48:51,009 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 16:48:51,200 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 16:48:51,375 - INFO - 表 dim_product_category 结构同步成功
2025-08-07 16:48:51,803 - INFO - 表 dim_product_category 数据同步完成，记录数: 8
2025-08-07 16:48:51,970 - INFO - 表 dim_traffic_source 结构同步成功
2025-08-07 16:48:52,106 - INFO - 表 dim_traffic_source 数据同步完成，记录数: 6
2025-08-07 16:48:52,193 - WARNING - 表 dwd_product_sale_detail 中发现重复列: dt，已跳过
2025-08-07 16:48:52,261 - INFO - 表 dwd_product_sale_detail 结构同步成功
2025-08-07 16:48:52,906 - INFO - 表 dwd_product_sale_detail 数据同步完成，记录数: 718
2025-08-07 16:48:52,987 - WARNING - 表 dwd_product_search_detail 中发现重复列: dt，已跳过
2025-08-07 16:48:53,071 - INFO - 表 dwd_product_search_detail 结构同步成功
2025-08-07 16:48:53,918 - INFO - 表 dwd_product_search_detail 数据同步完成，记录数: 1138
2025-08-07 16:48:54,006 - WARNING - 表 dwd_product_visit_detail 中发现重复列: dt，已跳过
2025-08-07 16:48:54,081 - INFO - 表 dwd_product_visit_detail 结构同步成功
2025-08-07 16:48:55,617 - INFO - 表 dwd_product_visit_detail 数据同步完成，记录数: 2241
2025-08-07 16:48:55,695 - WARNING - 表 dws_product_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:48:55,782 - INFO - 表 dws_product_sale_summary 结构同步成功
2025-08-07 16:48:56,158 - INFO - 表 dws_product_sale_summary 数据同步完成，记录数: 327
2025-08-07 16:48:56,240 - WARNING - 表 dws_product_visit_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:48:56,339 - INFO - 表 dws_product_visit_summary 结构同步成功
2025-08-07 16:48:56,539 - INFO - 表 dws_product_visit_summary 数据同步完成，记录数: 0
2025-08-07 16:48:56,613 - WARNING - 表 dws_search_word_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:48:56,698 - INFO - 表 dws_search_word_summary 结构同步成功
2025-08-07 16:48:56,897 - ERROR - 同步表 dws_search_word_summary 数据失败: (1241, 'Operand should contain 1 column(s)')
2025-08-07 16:48:56,897 - ERROR - 表 dws_search_word_summary 同步失败，跳过继续处理其他表
2025-08-07 16:48:56,976 - WARNING - 表 dws_sku_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 16:48:57,055 - INFO - 表 dws_sku_sale_summary 结构同步成功
2025-08-07 16:48:57,519 - INFO - 表 dws_sku_sale_summary 数据同步完成，记录数: 503
2025-08-07 16:48:57,620 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-07 16:48:57,798 - INFO - 表 ods_product_info 结构同步成功
2025-08-07 16:49:00,054 - INFO - 表 ods_product_info 数据同步完成，记录数: 3100
2025-08-07 16:49:00,133 - WARNING - 表 ods_product_sale_info 中发现重复列: dt，已跳过
2025-08-07 16:49:00,196 - INFO - 表 ods_product_sale_info 结构同步成功
2025-08-07 16:49:02,521 - INFO - 表 ods_product_sale_info 数据同步完成，记录数: 3000
2025-08-07 16:49:02,612 - WARNING - 表 ods_search_log 中发现重复列: dt，已跳过
2025-08-07 16:49:02,695 - INFO - 表 ods_search_log 结构同步成功
2025-08-07 16:49:06,001 - INFO - 表 ods_search_log 数据同步完成，记录数: 5000
2025-08-07 16:49:06,091 - WARNING - 表 ods_shop_visit_log 中发现重复列: dt，已跳过
2025-08-07 16:49:06,176 - INFO - 表 ods_shop_visit_log 结构同步成功
2025-08-07 16:49:12,545 - INFO - 表 ods_shop_visit_log 数据同步完成，记录数: 10000
2025-08-07 16:49:13,019 - INFO - Closing down clientserver connection
2025-08-07 16:51:08,205 - INFO - 发现 2 张待同步表: ['ads_inventory_warning', 'ads_product_comprehensive_ranking']
2025-08-07 16:51:08,561 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 16:51:11,232 - INFO - 表 ads_inventory_warning 数据同步完成，记录数: 361
2025-08-07 16:51:11,330 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 16:51:11,403 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 16:51:13,146 - INFO - 表 ads_product_comprehensive_ranking 数据同步完成，记录数: 1135
2025-08-07 16:51:13,379 - INFO - Closing down clientserver connection
2025-08-07 16:52:23,700 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 16:52:23,925 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:52:24,044 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 16:52:26,416 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 16:52:26,840 - INFO - Closing down clientserver connection
2025-08-07 16:55:08,441 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 16:55:08,683 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:55:08,790 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 16:55:11,422 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 16:55:11,620 - INFO - Closing down clientserver connection
2025-08-07 16:56:39,343 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 16:56:39,608 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 16:56:39,757 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 16:56:42,491 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 30
2025-08-07 16:56:43,039 - INFO - Closing down clientserver connection
2025-08-07 17:07:03,168 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 17:07:03,444 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 17:07:03,570 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 17:07:04,750 - INFO - NumExpr defaulting to 16 threads.
2025-08-07 17:07:08,520 - INFO - 开始同步表 ads_traffic_effect_analysis 的 0 条数据
2025-08-07 17:07:08,522 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 17:07:09,060 - INFO - Closing down clientserver connection
2025-08-07 17:07:41,614 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 17:07:41,829 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 17:07:41,941 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 17:07:42,733 - INFO - NumExpr defaulting to 16 threads.
2025-08-07 17:07:46,124 - INFO - 开始同步表 ads_traffic_effect_analysis 的 0 条数据
2025-08-07 17:07:46,125 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 17:07:46,496 - INFO - Closing down clientserver connection
2025-08-07 17:21:01,049 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 17:21:01,271 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 17:21:01,386 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 17:21:03,820 - INFO - 开始同步表 ads_traffic_effect_analysis 的 0 条数据
2025-08-07 17:21:03,823 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 17:21:04,389 - INFO - Closing down clientserver connection
2025-08-07 17:24:36,387 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 17:24:36,610 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 17:24:36,726 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 17:24:36,855 - ERROR - 同步表 ads_traffic_effect_analysis 数据失败: name 'col' is not defined
2025-08-07 17:24:36,855 - ERROR - 表 ads_traffic_effect_analysis 同步失败，跳过继续处理其他表
2025-08-07 17:24:37,181 - INFO - Closing down clientserver connection
2025-08-07 17:26:08,406 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 17:26:08,634 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 17:26:08,730 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 17:26:11,192 - INFO - 开始同步表 ads_traffic_effect_analysis 的 0 条数据
2025-08-07 17:26:11,207 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 17:26:11,728 - INFO - Closing down clientserver connection
2025-08-07 19:37:15,799 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 19:37:16,044 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 19:37:16,174 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 19:37:19,058 - INFO - 开始同步表 ads_traffic_effect_analysis 的 0 条数据
2025-08-07 19:37:19,060 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 19:37:19,627 - INFO - Closing down clientserver connection
2025-08-07 19:40:56,578 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 19:40:56,807 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 19:40:56,929 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 19:40:59,632 - INFO - 开始同步表 ads_traffic_effect_analysis 的 0 条数据
2025-08-07 19:40:59,634 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 19:40:59,731 - INFO - Closing down clientserver connection
2025-08-07 19:42:17,890 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 19:42:18,116 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 19:42:18,223 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 19:42:21,353 - INFO - 开始同步表 ads_traffic_effect_analysis 的 0 条数据
2025-08-07 19:42:21,371 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 19:42:21,619 - INFO - Closing down clientserver connection
2025-08-07 19:44:17,904 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 19:44:18,132 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 19:44:18,241 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 19:44:20,895 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 30
2025-08-07 19:44:21,108 - INFO - Closing down clientserver connection
2025-08-07 19:46:42,969 - INFO - 发现 1 张待同步表: ['ads_traffic_effect_analysis']
2025-08-07 19:46:43,182 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 19:46:43,282 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 19:46:43,370 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 19:46:44,572 - INFO - NumExpr defaulting to 16 threads.
2025-08-07 19:46:47,560 - INFO - 表 ads_traffic_effect_analysis 数据同步完成，记录数: 0
2025-08-07 19:46:47,697 - INFO - Closing down clientserver connection
2025-08-07 19:49:30,309 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:30,562 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:30,780 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:30,975 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:31,182 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:31,375 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:31,552 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:31,753 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:31,936 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:32,111 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:32,302 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:32,468 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:32,631 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:32,798 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:33,001 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:33,180 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:33,357 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:33,519 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:33,696 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:33,883 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:34,054 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:34,221 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:34,405 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:34,590 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:34,744 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:34,922 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:35,089 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:35,253 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:35,416 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:35,574 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:35,727 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:35,922 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:36,072 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:36,220 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:36,366 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:36,534 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:36,703 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:36,849 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:37,025 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:37,213 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:37,377 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:37,526 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:37,678 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:37,848 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:38,007 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:38,248 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:38,407 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:38,567 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:38,711 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:38,885 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:39,051 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:39,221 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:39,384 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:39,554 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:39,701 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:39,843 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:40,004 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:40,151 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:40,293 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:40,437 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:40,604 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:40,750 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:40,892 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:41,056 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:41,223 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:41,368 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:41,508 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:41,666 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:41,806 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:41,948 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:42,084 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:42,251 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:42,421 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:42,590 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:42,726 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:42,881 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:43,027 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:43,188 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:43,327 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:43,466 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:43,641 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:43,798 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:43,942 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:44,081 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:44,236 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:44,403 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:44,544 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:44,682 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:44,838 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:45,170 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:45,725 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:45,878 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:46,026 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:46,181 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:46,318 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:46,492 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:46,633 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:46,768 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:46,908 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:47,066 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:47,205 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:47,345 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:47,494 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:47,668 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:47,808 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:47,948 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:48,097 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:48,233 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:48,374 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:48,516 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:48,682 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:48,821 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:48,962 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:49,122 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:49,261 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:49,414 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:49,555 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:49,714 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:49,865 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:50,001 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:50,143 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:50,278 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:50,424 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:50,570 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:50,730 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:50,919 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:51,067 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:51,227 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:51,378 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:51,519 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:51,661 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:51,837 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:51,980 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:52,135 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:52,279 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:52,429 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:52,576 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:52,737 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:52,909 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:53,071 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:53,214 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:53,358 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:53,503 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:53,641 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:53,799 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:53,947 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:54,105 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:54,240 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:54,392 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:54,539 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:54,677 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:54,808 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:54,955 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:55,110 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:55,250 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:55,383 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:55,554 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:55,690 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:55,843 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:55,979 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:56,118 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:56,271 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:56,408 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:56,561 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:56,699 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:56,852 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:57,005 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:57,147 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:57,298 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:57,431 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:57,577 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:57,733 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:57,872 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:58,035 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:58,197 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:58,354 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:58,509 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:58,661 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:58,801 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:58,945 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:59,101 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:59,238 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:59,395 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:59,559 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:59,703 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:59,860 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:49:59,994 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:00,132 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:00,267 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:00,414 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:00,547 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:00,720 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:00,883 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:01,013 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:01,155 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:01,289 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:01,441 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:01,574 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:01,712 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:01,860 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:02,014 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:02,150 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:02,301 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:02,435 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:02,589 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:02,729 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:02,865 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:03,035 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:03,173 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:03,320 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:03,470 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:03,619 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:03,759 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:03,896 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:04,039 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:04,209 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:04,343 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:04,479 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:04,612 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:04,761 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:04,897 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:05,047 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:05,186 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:05,353 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:05,493 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:05,644 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:05,798 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:05,947 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:06,080 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:06,234 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:06,368 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:06,514 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:06,650 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:06,800 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:06,931 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:07,081 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:07,240 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:07,399 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:07,552 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:07,715 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:07,886 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:08,079 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:08,289 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:08,429 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:08,566 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:08,718 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:08,851 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:09,020 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:09,170 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:09,321 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:09,475 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:09,608 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:09,745 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:09,903 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:10,046 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:10,220 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:10,361 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:10,565 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:10,697 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:10,845 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:10,993 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:11,128 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:11,282 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:11,429 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:11,556 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:11,709 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:11,855 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:11,988 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:12,124 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:12,286 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:12,433 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:12,567 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:12,745 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:12,904 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:13,044 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:13,192 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:13,324 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:13,475 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:13,612 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:13,751 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:13,921 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:14,077 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:14,224 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:14,380 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:14,525 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:14,662 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:14,817 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:14,958 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:15,109 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:15,261 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:15,397 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:15,550 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:15,701 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:15,853 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:16,006 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:16,144 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:16,304 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:16,439 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:16,593 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:16,726 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:16,854 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:16,985 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:17,115 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:17,257 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:17,404 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:17,540 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:17,687 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:17,849 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:17,986 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:18,129 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:18,275 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:18,440 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:18,615 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:18,757 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:18,898 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:19,429 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:19,564 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:19,717 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:19,848 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:19,995 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:20,151 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:20,290 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:20,445 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:20,584 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:20,738 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:20,898 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:21,038 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:21,179 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:21,343 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:21,485 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:21,623 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:21,758 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:21,889 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:22,039 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:22,180 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:22,326 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:22,487 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:22,641 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:22,781 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:22,928 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:23,085 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:23,233 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:23,385 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:23,545 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:23,701 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:23,842 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:24,000 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:24,149 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:24,298 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:24,488 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:24,668 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:24,822 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:25,004 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:25,165 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:25,315 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:25,564 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:25,727 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:25,883 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:26,036 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:26,194 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:26,371 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:26,519 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:26,679 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:26,832 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:26,975 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:27,121 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:27,267 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:27,404 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:27,565 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:27,725 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:27,880 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:28,082 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:28,232 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:28,381 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:28,535 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:28,680 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:28,852 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:29,009 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:29,158 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:29,318 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:29,474 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:29,620 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:29,764 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:29,912 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:30,074 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:30,211 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:30,347 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:30,481 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:30,623 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:30,764 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:30,901 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:31,037 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:31,184 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:31,334 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:31,483 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:31,636 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:31,766 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:31,906 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:32,035 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:32,174 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:32,339 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:32,495 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:32,643 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:32,776 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:32,936 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:33,093 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:33,226 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:33,377 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:33,534 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:33,682 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:33,822 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:33,959 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:34,088 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:34,229 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:34,375 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:34,517 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:34,669 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:34,827 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:34,983 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:35,121 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:35,258 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:35,416 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:35,549 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:35,694 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:35,860 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:35,995 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:36,145 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:36,277 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:36,414 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:36,561 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:36,722 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:36,866 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:37,000 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:37,150 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:37,282 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:37,423 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:37,575 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:37,713 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:37,880 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:38,058 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:38,270 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:38,435 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:38,597 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:38,753 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:38,896 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:39,064 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:39,206 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:39,361 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:39,524 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:39,676 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:39,811 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:39,963 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:40,098 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:40,241 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:40,402 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:40,616 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:40,766 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:40,918 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:41,057 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:41,214 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:41,363 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:41,509 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:41,642 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:41,791 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:41,926 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:42,057 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:42,224 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:42,367 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:42,520 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:42,669 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:42,819 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:42,955 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:43,113 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:43,258 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:43,421 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:43,570 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:43,713 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:43,865 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:44,006 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:44,150 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:44,305 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:44,457 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:44,610 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:44,752 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:44,910 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:45,131 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:45,621 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:45,779 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:45,917 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:46,053 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:46,226 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:46,382 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:46,543 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:46,681 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:46,821 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:46,976 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:47,113 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:47,257 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:47,419 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:47,591 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:47,744 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:47,899 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:48,037 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:48,180 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:48,326 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:48,483 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:48,642 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:48,799 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:48,938 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:49,082 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:49,237 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:49,375 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:49,528 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:49,654 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:49,781 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:49,929 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:50,075 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:50,203 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:50,348 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:50,494 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:50,621 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:50,747 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:50,925 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:51,090 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:51,223 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:51,379 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:51,536 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:51,675 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:51,815 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:51,949 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:52,088 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:52,255 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:52,400 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:52,544 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:52,691 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:52,836 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:52,980 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:53,139 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:53,280 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:53,427 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:53,588 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:53,717 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:53,847 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:53,998 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:54,129 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:54,266 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:54,418 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:54,573 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:54,724 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:54,866 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:54,998 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:55,129 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:55,258 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:55,402 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:55,583 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:55,728 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:55,878 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:56,024 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:56,155 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:56,302 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:56,451 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:56,595 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:56,723 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:56,852 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:57,002 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:57,134 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:57,263 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:57,408 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:57,563 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:57,691 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:57,830 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:57,991 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:58,124 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:58,284 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:58,426 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:58,590 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:58,727 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:58,882 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:59,022 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:59,159 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:59,300 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:59,471 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:59,614 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:59,761 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:50:59,908 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:00,049 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:00,176 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:00,318 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:00,451 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:00,608 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:00,743 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:00,886 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:01,032 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:01,176 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:01,319 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:01,463 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:01,615 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:01,766 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:01,917 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:02,049 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:02,184 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:02,319 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:02,457 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:02,595 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:02,721 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:02,862 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:03,038 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:03,173 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:03,323 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:03,463 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:03,612 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:03,747 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:03,888 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:04,028 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:04,204 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:04,360 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:04,517 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:04,662 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:04,819 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:04,961 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:05,109 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:05,258 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:05,408 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:05,569 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:05,728 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:05,864 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:06,019 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:06,171 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:06,307 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:06,460 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:06,609 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:06,761 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:06,913 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:07,047 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:07,198 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:07,332 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:07,486 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:07,630 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:07,767 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:07,955 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:08,145 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:08,355 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:08,501 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:08,653 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:08,812 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:08,973 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:09,130 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:09,290 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:09,453 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:09,606 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:09,748 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:09,891 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:10,032 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:10,171 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:10,311 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:10,442 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:10,619 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:10,758 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:10,888 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:11,023 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:11,153 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:11,300 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:11,449 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:11,581 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:11,745 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:11,895 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:12,043 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:12,169 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:12,321 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:12,454 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:12,581 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:12,736 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:12,895 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:13,041 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:13,188 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:13,332 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:13,475 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:13,618 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:13,760 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:13,903 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:14,066 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:14,214 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:14,360 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:14,514 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:14,679 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:14,841 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:14,999 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:15,134 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:15,298 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:15,445 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:15,589 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:15,737 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:15,881 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:16,011 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:16,152 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:16,285 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:16,422 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:16,567 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:16,705 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:16,838 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:16,987 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:17,117 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:17,260 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:17,392 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:17,518 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:17,677 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:17,819 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:17,964 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:18,107 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:18,237 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:18,423 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:18,568 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:18,702 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:18,842 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:18,970 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:19,115 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:19,256 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:19,398 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:19,545 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:19,689 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:19,825 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:19,969 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:20,109 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:20,235 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:20,369 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:20,514 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:20,657 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:20,808 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:20,950 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:21,097 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:21,235 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:21,379 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:21,507 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:21,635 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:21,760 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:21,900 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:22,022 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:22,166 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:22,289 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:22,437 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:22,577 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:22,727 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:22,872 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:23,017 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:23,160 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:23,320 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:23,475 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:23,633 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:23,778 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:23,924 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:24,059 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:24,216 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:24,371 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:24,551 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:24,708 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:24,867 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:25,007 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:25,146 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:25,304 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:25,455 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:25,633 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:25,785 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:25,938 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:26,089 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:26,236 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:26,392 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:26,550 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:26,703 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:26,861 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:27,006 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:27,134 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:27,259 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:27,395 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:27,536 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:27,672 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:27,808 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:27,960 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:28,114 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:28,269 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:28,413 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:28,569 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:28,734 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:28,888 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:29,023 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:29,152 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:29,315 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:29,456 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:29,601 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:29,728 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:29,881 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:30,027 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:30,159 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:30,288 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:30,436 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:30,583 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:30,712 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:30,872 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:31,006 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:31,159 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:31,293 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:31,435 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:31,585 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:31,750 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:31,885 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:32,034 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:32,182 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:32,329 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:32,480 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:32,616 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:32,747 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:32,906 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:33,047 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:33,193 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:33,341 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:33,489 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:33,623 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:33,752 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:33,890 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:34,049 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:34,182 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:34,336 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:34,471 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:34,620 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:34,752 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:34,907 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:35,053 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:35,197 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:35,325 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:35,468 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:35,594 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:35,736 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:35,879 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:36,010 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:36,147 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:36,298 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:36,445 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:36,589 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:36,719 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:36,874 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:37,020 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:37,162 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:37,309 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:37,439 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:37,583 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:37,708 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:37,854 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:38,015 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:38,161 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:38,372 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:38,519 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:38,659 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:38,852 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:39,001 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:39,158 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:39,320 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:39,483 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:39,643 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:39,803 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:39,964 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:40,138 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:40,324 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:40,493 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:40,711 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:40,984 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:41,142 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:41,308 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:41,472 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:41,649 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:41,809 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:41,970 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:42,109 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:42,251 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:42,389 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:42,547 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:42,687 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:42,847 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:43,005 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:43,145 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:43,298 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:43,434 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:43,592 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:43,740 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:43,895 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:44,039 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:44,199 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:44,348 - INFO - 表 ads_traffic_effect_analysis 的Parquet文件已修复，add_cart_rate 字段物理类型转为double
2025-08-07 19:51:44,382 - INFO - Error while receiving.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=676>
2025-08-07 19:51:44,384 - INFO - Closing down clientserver connection
2025-08-07 19:51:44,384 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=676>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-08-07 19:51:44,387 - INFO - Error while receiving.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o51.sc
2025-08-07 19:51:44,392 - INFO - Closing down clientserver connection
2025-08-07 19:51:44,392 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o51.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-08-07 19:51:44,393 - INFO - Closing down clientserver connection
2025-08-07 19:51:44,407 - INFO - Closing down clientserver connection
2025-08-07 20:00:28,706 - INFO - 发现 16 张待同步表
2025-08-07 20:00:29,019 - INFO - 表 ads_inventory_warning 结构同步成功
2025-08-07 20:00:31,492 - INFO - 表 ads_inventory_warning 数据同步完成，记录数: 277
2025-08-07 20:00:31,587 - WARNING - 表 ads_product_comprehensive_ranking 中发现重复列: stat_period，已跳过
2025-08-07 20:00:31,684 - INFO - 表 ads_product_comprehensive_ranking 结构同步成功
2025-08-07 20:00:33,826 - INFO - 表 ads_product_comprehensive_ranking 数据同步完成，记录数: 1820
2025-08-07 20:00:33,909 - WARNING - 表 ads_traffic_effect_analysis 中发现重复列: stat_period，已跳过
2025-08-07 20:00:33,993 - INFO - 表 ads_traffic_effect_analysis 结构同步成功
2025-08-07 20:00:34,311 - ERROR - 同步表 ads_traffic_effect_analysis 数据失败: An error occurred while calling o79.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 13) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-4a3fc73e-e502-41f2-8e8b-2d6367f12ef6.c000.snappy.parquet. Column: [add_cart_rate], Expected: bigint, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: bigint
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_traffic_effect_analysis/stat_period=7day/part-00000-4a3fc73e-e502-41f2-8e8b-2d6367f12ef6.c000.snappy.parquet. Column: [add_cart_rate], Expected: bigint, Found: INT32.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [add_cart_rate], physicalType: INT32, logicalType: bigint
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-07 20:00:34,315 - ERROR - 表 ads_traffic_effect_analysis 同步失败，跳过继续处理其他表
2025-08-07 20:00:34,489 - INFO - 表 dim_product_category 结构同步成功
2025-08-07 20:00:34,784 - INFO - 表 dim_product_category 数据同步完成，记录数: 8
2025-08-07 20:00:34,936 - INFO - 表 dim_traffic_source 结构同步成功
2025-08-07 20:00:35,059 - INFO - 表 dim_traffic_source 数据同步完成，记录数: 6
2025-08-07 20:00:35,166 - WARNING - 表 dwd_product_sale_detail 中发现重复列: dt，已跳过
2025-08-07 20:00:35,232 - INFO - 表 dwd_product_sale_detail 结构同步成功
2025-08-07 20:00:35,888 - INFO - 表 dwd_product_sale_detail 数据同步完成，记录数: 718
2025-08-07 20:00:35,968 - WARNING - 表 dwd_product_search_detail 中发现重复列: dt，已跳过
2025-08-07 20:00:36,033 - INFO - 表 dwd_product_search_detail 结构同步成功
2025-08-07 20:00:36,912 - INFO - 表 dwd_product_search_detail 数据同步完成，记录数: 1138
2025-08-07 20:00:37,012 - WARNING - 表 dwd_product_visit_detail 中发现重复列: dt，已跳过
2025-08-07 20:00:37,070 - INFO - 表 dwd_product_visit_detail 结构同步成功
2025-08-07 20:00:38,597 - INFO - 表 dwd_product_visit_detail 数据同步完成，记录数: 2241
2025-08-07 20:00:38,697 - WARNING - 表 dws_product_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 20:00:38,903 - INFO - 表 dws_product_sale_summary 结构同步成功
2025-08-07 20:00:39,271 - INFO - 表 dws_product_sale_summary 数据同步完成，记录数: 327
2025-08-07 20:00:39,357 - WARNING - 表 dws_product_visit_summary 中发现重复列: stat_period，已跳过
2025-08-07 20:00:39,478 - INFO - 表 dws_product_visit_summary 结构同步成功
2025-08-07 20:00:39,811 - ERROR - 同步表 dws_product_visit_summary 数据失败: An error occurred while calling o172.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 9.0 failed 1 times, most recent failure: Lost task 1.0 in stage 9.0 (TID 45) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-d58dd38e-cfa4-4b4c-a357-b333deabe7a0.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-d58dd38e-cfa4-4b4c-a357-b333deabe7a0.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-07 20:00:39,816 - ERROR - 表 dws_product_visit_summary 同步失败，跳过继续处理其他表
2025-08-07 20:00:39,898 - WARNING - 表 dws_search_word_summary 中发现重复列: stat_period，已跳过
2025-08-07 20:00:39,988 - INFO - 表 dws_search_word_summary 结构同步成功
2025-08-07 20:00:40,183 - ERROR - 同步表 dws_search_word_summary 数据失败: (1241, 'Operand should contain 1 column(s)')
2025-08-07 20:00:40,183 - ERROR - 表 dws_search_word_summary 同步失败，跳过继续处理其他表
2025-08-07 20:00:40,275 - WARNING - 表 dws_sku_sale_summary 中发现重复列: stat_period，已跳过
2025-08-07 20:00:40,340 - INFO - 表 dws_sku_sale_summary 结构同步成功
2025-08-07 20:00:40,827 - INFO - 表 dws_sku_sale_summary 数据同步完成，记录数: 503
2025-08-07 20:00:40,922 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-07 20:00:41,016 - INFO - 表 ods_product_info 结构同步成功
2025-08-07 20:00:43,333 - INFO - 表 ods_product_info 数据同步完成，记录数: 3100
2025-08-07 20:00:43,417 - WARNING - 表 ods_product_sale_info 中发现重复列: dt，已跳过
2025-08-07 20:00:43,498 - INFO - 表 ods_product_sale_info 结构同步成功
2025-08-07 20:00:46,155 - INFO - 表 ods_product_sale_info 数据同步完成，记录数: 3000
2025-08-07 20:00:46,244 - WARNING - 表 ods_search_log 中发现重复列: dt，已跳过
2025-08-07 20:00:46,333 - INFO - 表 ods_search_log 结构同步成功
2025-08-07 20:00:49,638 - INFO - 表 ods_search_log 数据同步完成，记录数: 5000
2025-08-07 20:00:49,718 - WARNING - 表 ods_shop_visit_log 中发现重复列: dt，已跳过
2025-08-07 20:00:49,786 - INFO - 表 ods_shop_visit_log 结构同步成功
2025-08-07 20:00:56,145 - INFO - 表 ods_shop_visit_log 数据同步完成，记录数: 10000
2025-08-07 20:00:56,474 - INFO - Closing down clientserver connection
2025-08-08 10:28:16,043 - INFO - 发现 17 张待同步表
2025-08-08 10:28:16,271 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:28:16,396 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 10:28:18,986 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 40
2025-08-08 10:28:19,094 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:28:19,192 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 10:28:19,593 - ERROR - 同步表 ads_hot_product_ranking 数据失败: An error occurred while calling o66.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 6) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Column: [total_visit_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_visit_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Column: [total_visit_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_visit_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-08 10:28:19,597 - ERROR - 表 ads_hot_product_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:28:19,695 - WARNING - 表 ads_hot_search_word 中发现重复列: stat_period，已跳过
2025-08-08 10:28:19,810 - INFO - 表 ads_hot_search_word 结构同步成功
2025-08-08 10:28:20,294 - ERROR - 同步表 ads_hot_search_word 数据失败: An error occurred while calling o81.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 11) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_search_word/stat_period=day/part-00000-4650c70a-e964-4d12-bd16-425ea7f5d561.c000.snappy.parquet. Column: [total_search_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_search_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_search_word/stat_period=day/part-00000-4650c70a-e964-4d12-bd16-425ea7f5d561.c000.snappy.parquet. Column: [total_search_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_search_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-08 10:28:20,296 - ERROR - 表 ads_hot_search_word 同步失败，跳过继续处理其他表
2025-08-08 10:28:20,398 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 10:28:20,492 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 10:28:20,811 - ERROR - 同步表 ads_platform_core_index 数据失败: An error occurred while calling o96.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 1 times, most recent failure: Lost task 2.0 in stage 3.0 (TID 17) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-7cc3365b-97fd-4980-9b64-49245c33a762.c000.snappy.parquet. Column: [total_visit_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_visit_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-7cc3365b-97fd-4980-9b64-49245c33a762.c000.snappy.parquet. Column: [total_visit_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_visit_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-08 10:28:20,817 - ERROR - 表 ads_platform_core_index 同步失败，跳过继续处理其他表
2025-08-08 10:28:20,973 - INFO - 表 dim_product_category 结构同步成功
2025-08-08 10:28:21,426 - INFO - 表 dim_product_category 数据同步完成，记录数: 8
2025-08-08 10:28:21,605 - INFO - 表 dim_traffic_source 结构同步成功
2025-08-08 10:28:21,743 - INFO - 表 dim_traffic_source 数据同步完成，记录数: 6
2025-08-08 10:28:21,833 - WARNING - 表 dwd_product_sale_detail 中发现重复列: dt，已跳过
2025-08-08 10:28:21,917 - INFO - 表 dwd_product_sale_detail 结构同步成功
2025-08-08 10:28:22,576 - INFO - 表 dwd_product_sale_detail 数据同步完成，记录数: 718
2025-08-08 10:28:22,674 - WARNING - 表 dwd_product_search_detail 中发现重复列: dt，已跳过
2025-08-08 10:28:22,754 - INFO - 表 dwd_product_search_detail 结构同步成功
2025-08-08 10:28:23,650 - INFO - 表 dwd_product_search_detail 数据同步完成，记录数: 1138
2025-08-08 10:28:23,732 - WARNING - 表 dwd_product_visit_detail 中发现重复列: dt，已跳过
2025-08-08 10:28:23,815 - INFO - 表 dwd_product_visit_detail 结构同步成功
2025-08-08 10:28:25,354 - INFO - 表 dwd_product_visit_detail 数据同步完成，记录数: 2241
2025-08-08 10:28:25,443 - WARNING - 表 dws_product_sale_summary 中发现重复列: stat_period，已跳过
2025-08-08 10:28:25,534 - INFO - 表 dws_product_sale_summary 结构同步成功
2025-08-08 10:28:25,904 - INFO - 表 dws_product_sale_summary 数据同步完成，记录数: 327
2025-08-08 10:28:25,983 - WARNING - 表 dws_product_visit_summary 中发现重复列: stat_period，已跳过
2025-08-08 10:28:26,061 - INFO - 表 dws_product_visit_summary 结构同步成功
2025-08-08 10:28:26,361 - ERROR - 同步表 dws_product_visit_summary 数据失败: An error occurred while calling o189.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 10.0 failed 1 times, most recent failure: Lost task 2.0 in stage 10.0 (TID 50) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-f1bc9280-bee0-4053-9c45-45f2a0e5be8e.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_product_visit_summary/stat_period=day/part-00000-f1bc9280-bee0-4053-9c45-45f2a0e5be8e.c000.snappy.parquet. Column: [source_visitor_map, key_value, value], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [source_visitor_map, key_value, value], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-08 10:28:26,366 - ERROR - 表 dws_product_visit_summary 同步失败，跳过继续处理其他表
2025-08-08 10:28:26,453 - WARNING - 表 dws_search_word_summary 中发现重复列: stat_period，已跳过
2025-08-08 10:28:26,556 - INFO - 表 dws_search_word_summary 结构同步成功
2025-08-08 10:28:26,767 - ERROR - 同步表 dws_search_word_summary 数据失败: (1241, 'Operand should contain 1 column(s)')
2025-08-08 10:28:26,768 - ERROR - 表 dws_search_word_summary 同步失败，跳过继续处理其他表
2025-08-08 10:28:26,874 - WARNING - 表 dws_sku_sale_summary 中发现重复列: stat_period，已跳过
2025-08-08 10:28:26,971 - INFO - 表 dws_sku_sale_summary 结构同步成功
2025-08-08 10:28:27,251 - ERROR - 同步表 dws_sku_sale_summary 数据失败: An error occurred while calling o217.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 12.0 failed 1 times, most recent failure: Lost task 3.0 in stage 12.0 (TID 61) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_sku_sale_summary/stat_period=day/part-00000-d3e0fe0f-efb1-4ac8-a371-c29c27790771.c000.snappy.parquet. Column: [total_pay_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_pay_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/dws/dws_sku_sale_summary/stat_period=day/part-00000-d3e0fe0f-efb1-4ac8-a371-c29c27790771.c000.snappy.parquet. Column: [total_pay_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_pay_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-08 10:28:27,256 - ERROR - 表 dws_sku_sale_summary 同步失败，跳过继续处理其他表
2025-08-08 10:28:27,340 - WARNING - 表 ods_product_info 中发现重复列: dt，已跳过
2025-08-08 10:28:27,439 - INFO - 表 ods_product_info 结构同步成功
2025-08-08 10:28:29,673 - INFO - 表 ods_product_info 数据同步完成，记录数: 3100
2025-08-08 10:28:29,775 - WARNING - 表 ods_product_sale_info 中发现重复列: dt，已跳过
2025-08-08 10:28:29,856 - INFO - 表 ods_product_sale_info 结构同步成功
2025-08-08 10:28:32,024 - INFO - 表 ods_product_sale_info 数据同步完成，记录数: 3000
2025-08-08 10:28:32,098 - WARNING - 表 ods_search_log 中发现重复列: dt，已跳过
2025-08-08 10:28:32,202 - INFO - 表 ods_search_log 结构同步成功
2025-08-08 10:28:35,522 - INFO - 表 ods_search_log 数据同步完成，记录数: 5000
2025-08-08 10:28:35,613 - WARNING - 表 ods_shop_visit_log 中发现重复列: dt，已跳过
2025-08-08 10:28:35,683 - INFO - 表 ods_shop_visit_log 结构同步成功
2025-08-08 10:28:41,917 - INFO - 表 ods_shop_visit_log 数据同步完成，记录数: 10000
2025-08-08 10:28:42,152 - INFO - Closing down clientserver connection
2025-08-08 10:31:19,596 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_ranking, ads_platform_core_index
2025-08-08 10:31:22,869 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:31:22,990 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 10:31:25,202 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 40
2025-08-08 10:31:25,299 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:31:25,395 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 10:31:25,735 - ERROR - 同步表 ads_hot_product_ranking 数据失败: An error occurred while calling o62.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 6) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Column: [total_visit_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_visit_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Column: [total_visit_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_visit_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-08 10:31:25,738 - ERROR - 表 ads_hot_product_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:31:25,767 - ERROR - 表 ads_hot_search_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:31:25,867 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 10:31:25,939 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 10:31:26,229 - ERROR - 同步表 ads_platform_core_index 数据失败: An error occurred while calling o80.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 10) (10.163.47.55 executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-15158128-6fa4-430d-9299-14e0f5259b80.c000.snappy.parquet. Column: [total_visit_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_visit_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-15158128-6fa4-430d-9299-14e0f5259b80.c000.snappy.parquet. Column: [total_visit_num], Expected: int, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:855)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [total_visit_num], physicalType: INT64, logicalType: int
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 23 more

2025-08-08 10:31:26,231 - ERROR - 表 ads_platform_core_index 同步失败，跳过继续处理其他表
2025-08-08 10:31:26,606 - INFO - Closing down clientserver connection
2025-08-08 10:32:36,490 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_ranking, ads_platform_core_index
2025-08-08 10:32:39,903 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:32:40,086 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 10:32:40,272 - ERROR - 同步表 ads_category_sale_ranking 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sales_amount` cannot be resolved. Did you mean one of the following? [`sales_rank`, `total_sales_amount`, `category_id`, `stat_date`, `stat_period`].; line 6 pos 24;
'Project [stat_date#28, category_id#22, category_name#23, 'sales_amount, 'sales_num, 'sales_ratio, 'rank, stat_period#29]
+- SubqueryAlias spark_catalog.gd02.ads_category_sale_ranking
   +- Relation spark_catalog.gd02.ads_category_sale_ranking[category_id#22,category_name#23,total_sales_amount#24,total_sales_num#25L,sales_rank#26,sales_contribution#27,stat_date#28,stat_period#29] parquet

2025-08-08 10:32:40,272 - ERROR - 表 ads_category_sale_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:32:40,377 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:32:40,453 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 10:32:40,531 - ERROR - 同步表 ads_hot_product_ranking 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `category_name` cannot be resolved. Did you mean one of the following? [`category_id`, `product_name`, `stat_date`, `hot_rank`, `conversion_rate`].; line 6 pos 24;
'Project [stat_date#69, product_id#62, product_name#63, 'category_name, 'sales_amount, cast('visit_num as bigint) AS visit_num#61, conversion_rate#67, 'rank, stat_period#70]
+- SubqueryAlias spark_catalog.gd02.ads_hot_product_ranking
   +- Relation spark_catalog.gd02.ads_hot_product_ranking[product_id#62,product_name#63,category_id#64,total_sales_amount#65,total_visit_num#66,conversion_rate#67,hot_rank#68,stat_date#69,stat_period#70] parquet

2025-08-08 10:32:40,532 - ERROR - 表 ads_hot_product_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:32:40,557 - ERROR - 表 ads_hot_search_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:32:40,647 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 10:32:40,723 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 10:32:40,795 - ERROR - 同步表 ads_platform_core_index 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `total_sales_num` cannot be resolved. Did you mean one of the following? [`total_order_num`, `total_sales_amount`, `total_visit_num`, `total_visitor_num`, `stat_date`].; line 8 pos 29;
'Project [stat_date#112, cast(total_visitor_num#113L as bigint) AS total_visitor_num#108L, cast(total_visit_num#114 as bigint) AS total_visit_num#109L, cast(total_order_num#115 as bigint) AS total_order_num#110L, total_sales_amount#116, cast('total_sales_num as bigint) AS total_sales_num#111, avg_order_amount#117, pay_conversion_rate#118, 'visitor_avg_visit_num, yoy_growth_rate#120, mom_growth_rate#121, stat_period#122]
+- SubqueryAlias spark_catalog.gd02.ads_platform_core_index
   +- Relation spark_catalog.gd02.ads_platform_core_index[stat_date#112,total_visitor_num#113L,total_visit_num#114,total_order_num#115,total_sales_amount#116,avg_order_amount#117,pay_conversion_rate#118,visitor_avg_stay_time#119,yoy_growth_rate#120,mom_growth_rate#121,stat_period#122] parquet

2025-08-08 10:32:40,795 - ERROR - 表 ads_platform_core_index 同步失败，跳过继续处理其他表
2025-08-08 10:32:40,961 - INFO - Closing down clientserver connection
2025-08-08 10:34:46,147 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_ranking, ads_platform_core_index
2025-08-08 10:34:49,399 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:34:49,538 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 10:34:51,832 - ERROR - 同步表 ads_category_sale_ranking 数据失败: (1054, "Unknown column 'sales_amount' in 'field list'")
2025-08-08 10:34:51,832 - ERROR - 表 ads_category_sale_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:34:51,932 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:34:52,014 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 10:34:52,408 - ERROR - 同步表 ads_hot_product_ranking 数据失败: An error occurred while calling o64.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 6) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:34:52,411 - ERROR - 表 ads_hot_product_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:34:52,438 - ERROR - 表 ads_hot_search_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:34:52,565 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 10:34:52,664 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 10:34:53,031 - ERROR - 同步表 ads_platform_core_index 数据失败: An error occurred while calling o83.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 2.0 failed 1 times, most recent failure: Lost task 4.0 in stage 2.0 (TID 14) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:34:53,035 - ERROR - 表 ads_platform_core_index 同步失败，跳过继续处理其他表
2025-08-08 10:34:53,171 - INFO - Closing down clientserver connection
2025-08-08 10:38:51,767 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_ranking, ads_platform_core_index
2025-08-08 10:38:55,066 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:38:55,196 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 10:38:57,577 - ERROR - 同步表 ads_category_sale_ranking 数据失败: (1054, "Unknown column 'sales_amount' in 'field list'")
2025-08-08 10:38:57,577 - ERROR - 表 ads_category_sale_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:38:57,669 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:38:57,740 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 10:38:58,178 - ERROR - 同步表 ads_hot_product_ranking 数据失败: An error occurred while calling o64.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 7) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-03ced996-73c7-48d7-b95d-bfdea6049e71.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-03ced996-73c7-48d7-b95d-bfdea6049e71.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-03ced996-73c7-48d7-b95d-bfdea6049e71.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-03ced996-73c7-48d7-b95d-bfdea6049e71.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:38:58,182 - ERROR - 表 ads_hot_product_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:38:58,213 - ERROR - 表 ads_hot_search_ranking 同步失败，跳过继续处理其他表
2025-08-08 10:38:58,309 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 10:38:58,384 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 10:38:58,756 - ERROR - 同步表 ads_platform_core_index 数据失败: An error occurred while calling o83.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 11) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-2e2d8964-7929-479e-92ab-562987bec99a.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-2e2d8964-7929-479e-92ab-562987bec99a.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-2e2d8964-7929-479e-92ab-562987bec99a.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-2e2d8964-7929-479e-92ab-562987bec99a.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:38:58,757 - ERROR - 表 ads_platform_core_index 同步失败，跳过继续处理其他表
2025-08-08 10:38:59,304 - INFO - Closing down clientserver connection
2025-08-08 10:41:04,840 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_ranking, ads_platform_core_index
2025-08-08 10:41:08,079 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:41:08,237 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 10:41:11,145 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 40
2025-08-08 10:41:11,241 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:41:11,329 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 10:41:11,731 - ERROR - 同步表 ads_hot_product_ranking 数据失败: An error occurred while calling o64.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 5) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-baf2077c-4984-4627-bc59-53d862b557ab.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-baf2077c-4984-4627-bc59-53d862b557ab.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-baf2077c-4984-4627-bc59-53d862b557ab.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-baf2077c-4984-4627-bc59-53d862b557ab.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:41:11,737 - ERROR - 表 ads_hot_product_ranking 同步失败: An error occurred while calling o64.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 5) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-baf2077c-4984-4627-bc59-53d862b557ab.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-baf2077c-4984-4627-bc59-53d862b557ab.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-baf2077c-4984-4627-bc59-53d862b557ab.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-baf2077c-4984-4627-bc59-53d862b557ab.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more
，跳过继续处理其他表
2025-08-08 10:41:11,774 - ERROR - 表 ads_hot_search_ranking 同步失败: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gd02`.`ads_hot_search_ranking` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;
'DescribeRelation false, [col_name#183, data_type#184, comment#185]
+- 'UnresolvedTableOrView [gd02, ads_hot_search_ranking], DESCRIBE TABLE, true
，跳过继续处理其他表
2025-08-08 10:41:11,873 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 10:41:11,975 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 10:41:12,351 - ERROR - 同步表 ads_platform_core_index 数据失败: An error occurred while calling o84.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 1 times, most recent failure: Lost task 3.0 in stage 2.0 (TID 13) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-526bf13d-3f85-4029-82ae-5224cff03d71.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-526bf13d-3f85-4029-82ae-5224cff03d71.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-526bf13d-3f85-4029-82ae-5224cff03d71.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-526bf13d-3f85-4029-82ae-5224cff03d71.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:41:12,352 - ERROR - 表 ads_platform_core_index 同步失败: An error occurred while calling o84.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 1 times, most recent failure: Lost task 3.0 in stage 2.0 (TID 13) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-526bf13d-3f85-4029-82ae-5224cff03d71.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-526bf13d-3f85-4029-82ae-5224cff03d71.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-526bf13d-3f85-4029-82ae-5224cff03d71.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-526bf13d-3f85-4029-82ae-5224cff03d71.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more
，跳过继续处理其他表
2025-08-08 10:41:12,885 - INFO - Closing down clientserver connection
2025-08-08 10:43:25,598 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_ranking, ads_platform_core_index
2025-08-08 10:43:28,793 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:43:28,913 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 10:43:31,304 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 40
2025-08-08 10:43:31,391 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:43:31,459 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 10:43:31,911 - ERROR - 同步表 ads_hot_product_ranking 数据失败: An error occurred while calling o95.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 6) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:43:31,915 - ERROR - 表 ads_hot_product_ranking 同步失败: An error occurred while calling o95.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 6) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-025caf95-d03d-48e1-a405-558bfc351847.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more
，跳过继续处理其他表
2025-08-08 10:43:31,965 - ERROR - 表 ads_hot_search_ranking 同步失败: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gd02`.`ads_hot_search_ranking` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;
'DescribeRelation false, [col_name#202, data_type#203, comment#204]
+- 'UnresolvedTableOrView [gd02, ads_hot_search_ranking], DESCRIBE TABLE, true
，跳过继续处理其他表
2025-08-08 10:43:32,078 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 10:43:32,165 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 10:43:32,543 - ERROR - 同步表 ads_platform_core_index 数据失败: An error occurred while calling o135.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 2.0 failed 1 times, most recent failure: Lost task 4.0 in stage 2.0 (TID 14) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:43:32,545 - ERROR - 表 ads_platform_core_index 同步失败: An error occurred while calling o135.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 2.0 failed 1 times, most recent failure: Lost task 4.0 in stage 2.0 (TID 14) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_platform_core_index/stat_period=day/part-00000-d29e586d-959c-4042-bc24-1b4ff15dbd7d.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more
，跳过继续处理其他表
2025-08-08 10:43:33,132 - INFO - Closing down clientserver connection
2025-08-08 10:59:05,464 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_ranking, ads_platform_core_index
2025-08-08 10:59:08,654 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:59:08,796 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 10:59:11,241 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 8
2025-08-08 10:59:11,335 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 10:59:11,428 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 10:59:11,842 - ERROR - 同步表 ads_hot_product_ranking 数据失败: An error occurred while calling o95.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-ab68b46a-92f9-4fb4-9dec-727e7ab8a940.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-ab68b46a-92f9-4fb4-9dec-727e7ab8a940.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-ab68b46a-92f9-4fb4-9dec-727e7ab8a940.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-ab68b46a-92f9-4fb4-9dec-727e7ab8a940.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

2025-08-08 10:59:11,845 - ERROR - 表 ads_hot_product_ranking 同步失败: An error occurred while calling o95.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (10.163.47.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-ab68b46a-92f9-4fb4-9dec-727e7ab8a940.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-ab68b46a-92f9-4fb4-9dec-727e7ab8a940.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-ab68b46a-92f9-4fb4-9dec-727e7ab8a940.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:296)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://cdh01:8020/warehouse/gd02/ads/ads_hot_product_ranking/stat_period=day/part-00000-ab68b46a-92f9-4fb4-9dec-727e7ab8a940.c000.snappy.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:210)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:286)
	... 22 more
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableInt cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setLong(SpecificInternalRow.scala:304)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setLong(ParquetRowConverter.scala:184)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addLong(ParquetRowConverter.scala:94)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:325)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:234)
	... 27 more
，跳过继续处理其他表
2025-08-08 10:59:11,883 - ERROR - 表 ads_hot_search_ranking 同步失败: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gd02`.`ads_hot_search_ranking` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;
'DescribeRelation false, [col_name#134, data_type#135, comment#136]
+- 'UnresolvedTableOrView [gd02, ads_hot_search_ranking], DESCRIBE TABLE, true
，跳过继续处理其他表
2025-08-08 10:59:11,977 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 10:59:12,052 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 10:59:12,311 - INFO - 表 ads_platform_core_index 数据同步完成，记录数: 1
2025-08-08 10:59:12,472 - INFO - Closing down clientserver connection
2025-08-08 11:02:02,910 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_ranking, ads_platform_core_index
2025-08-08 11:02:06,203 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 11:02:06,342 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 11:02:08,779 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 8
2025-08-08 11:02:08,862 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 11:02:08,945 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 11:02:09,555 - INFO - 表 ads_hot_product_ranking 数据同步完成，记录数: 95
2025-08-08 11:02:09,588 - ERROR - 表 ads_hot_search_ranking 同步失败: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gd02`.`ads_hot_search_ranking` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;
'DescribeRelation false, [col_name#134, data_type#135, comment#136]
+- 'UnresolvedTableOrView [gd02, ads_hot_search_ranking], DESCRIBE TABLE, true
，跳过继续处理其他表
2025-08-08 11:02:09,672 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 11:02:09,740 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 11:02:10,008 - INFO - 表 ads_platform_core_index 数据同步完成，记录数: 1
2025-08-08 11:02:10,436 - INFO - Closing down clientserver connection
2025-08-08 11:16:33,072 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_word, ads_platform_core_index
2025-08-08 11:16:36,413 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 11:16:36,537 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 11:16:38,930 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 8
2025-08-08 11:16:39,017 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 11:16:39,098 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 11:16:39,741 - INFO - 表 ads_hot_product_ranking 数据同步完成，记录数: 95
2025-08-08 11:16:39,820 - WARNING - 表 ads_hot_search_word 中发现重复列: stat_period，已跳过
2025-08-08 11:16:39,917 - INFO - 表 ads_hot_search_word 结构同步成功
2025-08-08 11:16:40,131 - ERROR - 同步表 ads_hot_search_word 数据失败: (1241, 'Operand should contain 1 column(s)')
2025-08-08 11:16:40,131 - ERROR - 表 ads_hot_search_word 同步失败: (1241, 'Operand should contain 1 column(s)')，跳过继续处理其他表
2025-08-08 11:16:40,215 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 11:16:40,320 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 11:16:40,562 - INFO - 表 ads_platform_core_index 数据同步完成，记录数: 1
2025-08-08 11:16:41,086 - INFO - Closing down clientserver connection
2025-08-08 11:20:46,579 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_word, ads_platform_core_index
2025-08-08 11:20:49,805 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 11:20:49,929 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 11:20:52,372 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 8
2025-08-08 11:20:52,455 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 11:20:52,538 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 11:20:53,158 - INFO - 表 ads_hot_product_ranking 数据同步完成，记录数: 95
2025-08-08 11:20:53,236 - WARNING - 表 ads_hot_search_word 中发现重复列: stat_period，已跳过
2025-08-08 11:20:53,329 - INFO - 表 ads_hot_search_word 结构同步成功
2025-08-08 11:20:53,704 - INFO - 表 ads_hot_search_word 数据同步完成，记录数: 20
2025-08-08 11:20:53,784 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 11:20:53,866 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 11:20:54,111 - INFO - 表 ads_platform_core_index 数据同步完成，记录数: 1
2025-08-08 11:20:54,599 - INFO - Closing down clientserver connection
2025-08-08 11:26:39,887 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_word, ads_platform_core_index
2025-08-08 11:26:43,021 - WARNING - 表 ads_category_sale_ranking 中发现重复列: stat_period，已跳过
2025-08-08 11:26:43,150 - INFO - 表 ads_category_sale_ranking 结构同步成功
2025-08-08 11:26:45,463 - INFO - 表 ads_category_sale_ranking 数据同步完成，记录数: 40
2025-08-08 11:26:45,549 - WARNING - 表 ads_hot_product_ranking 中发现重复列: stat_period，已跳过
2025-08-08 11:26:45,652 - INFO - 表 ads_hot_product_ranking 结构同步成功
2025-08-08 11:26:46,257 - INFO - 表 ads_hot_product_ranking 数据同步完成，记录数: 487
2025-08-08 11:26:46,351 - WARNING - 表 ads_hot_search_word 中发现重复列: stat_period，已跳过
2025-08-08 11:26:46,415 - INFO - 表 ads_hot_search_word 结构同步成功
2025-08-08 11:26:46,827 - INFO - 表 ads_hot_search_word 数据同步完成，记录数: 100
2025-08-08 11:26:46,919 - WARNING - 表 ads_platform_core_index 中发现重复列: stat_period，已跳过
2025-08-08 11:26:47,013 - INFO - 表 ads_platform_core_index 结构同步成功
2025-08-08 11:26:47,247 - INFO - 表 ads_platform_core_index 数据同步完成，记录数: 5
2025-08-09 08:46:19,229 - INFO - 准备同步 2 张表: ads_product_efficiency_monitor, ads_product_range_analysis
2025-08-09 08:46:23,512 - WARNING - 表 ads_product_efficiency_monitor 中发现重复列: dt，已跳过
2025-08-09 08:46:23,695 - INFO - 表 ads_product_efficiency_monitor 结构同步成功
2025-08-09 08:46:23,908 - ERROR - 同步表 ads_product_efficiency_monitor 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `stat_period` cannot be resolved. Did you mean one of the following? [`stat_date`, `dt`, `cart_quantity`, `cart_user_count`, `paid_amount`].;
'Filter ('stat_period = day)
+- SubqueryAlias spark_catalog.gd01.ads_product_efficiency_monitor
   +- Relation spark_catalog.gd01.ads_product_efficiency_monitor[stat_date#29,time_dimension#30,view_count#31L,visitor_count#32L,collect_count#33L,cart_quantity#34L,cart_user_count#35L,order_quantity#36L,order_amount#37,order_user_count#38L,paid_quantity#39L,paid_amount#40,paid_user_count#41L,conversion_rate#42,dt#43] parquet

2025-08-09 08:46:23,912 - ERROR - 表 ads_product_efficiency_monitor 同步失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `stat_period` cannot be resolved. Did you mean one of the following? [`stat_date`, `dt`, `cart_quantity`, `cart_user_count`, `paid_amount`].;
'Filter ('stat_period = day)
+- SubqueryAlias spark_catalog.gd01.ads_product_efficiency_monitor
   +- Relation spark_catalog.gd01.ads_product_efficiency_monitor[stat_date#29,time_dimension#30,view_count#31L,visitor_count#32L,collect_count#33L,cart_quantity#34L,cart_user_count#35L,order_quantity#36L,order_amount#37,order_user_count#38L,paid_quantity#39L,paid_amount#40,paid_user_count#41L,conversion_rate#42,dt#43] parquet
，跳过继续处理其他表
2025-08-09 08:46:24,044 - WARNING - 表 ads_product_range_analysis 中发现重复列: dt，已跳过
2025-08-09 08:46:24,153 - INFO - 表 ads_product_range_analysis 结构同步成功
2025-08-09 08:46:24,226 - ERROR - 同步表 ads_product_range_analysis 数据失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `stat_period` cannot be resolved. Did you mean one of the following? [`sort_metric`, `stat_date`, `avg_price`, `dt`, `paid_amount`].;
'Filter ('stat_period = day)
+- SubqueryAlias spark_catalog.gd01.ads_product_range_analysis
   +- Relation spark_catalog.gd01.ads_product_range_analysis[category_name#100,stat_date#101,time_dimension#102,range_type#103,range_value#104,product_count#105L,paid_amount#106,paid_quantity#107L,avg_price#108,sort_metric#109,dt#110] parquet

2025-08-09 08:46:24,229 - ERROR - 表 ads_product_range_analysis 同步失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `stat_period` cannot be resolved. Did you mean one of the following? [`sort_metric`, `stat_date`, `avg_price`, `dt`, `paid_amount`].;
'Filter ('stat_period = day)
+- SubqueryAlias spark_catalog.gd01.ads_product_range_analysis
   +- Relation spark_catalog.gd01.ads_product_range_analysis[category_name#100,stat_date#101,time_dimension#102,range_type#103,range_value#104,product_count#105L,paid_amount#106,paid_quantity#107L,avg_price#108,sort_metric#109,dt#110] parquet
，跳过继续处理其他表
2025-08-09 08:46:24,686 - INFO - Closing down clientserver connection
2025-08-09 08:50:26,072 - INFO - 准备同步 4 张表: ads_category_sale_ranking, ads_hot_product_ranking, ads_hot_search_word, ads_platform_core_index
2025-08-09 08:50:29,537 - ERROR - 表 ads_category_sale_ranking 同步失败: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gd01`.`ads_category_sale_ranking` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;
'DescribeRelation false, [col_name#0, data_type#1, comment#2]
+- 'UnresolvedTableOrView [gd01, ads_category_sale_ranking], DESCRIBE TABLE, true
，跳过继续处理其他表
2025-08-09 08:50:29,573 - ERROR - 表 ads_hot_product_ranking 同步失败: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gd01`.`ads_hot_product_ranking` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;
'DescribeRelation false, [col_name#3, data_type#4, comment#5]
+- 'UnresolvedTableOrView [gd01, ads_hot_product_ranking], DESCRIBE TABLE, true
，跳过继续处理其他表
2025-08-09 08:50:29,616 - ERROR - 表 ads_hot_search_word 同步失败: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gd01`.`ads_hot_search_word` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;
'DescribeRelation false, [col_name#6, data_type#7, comment#8]
+- 'UnresolvedTableOrView [gd01, ads_hot_search_word], DESCRIBE TABLE, true
，跳过继续处理其他表
2025-08-09 08:50:29,651 - ERROR - 表 ads_platform_core_index 同步失败: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gd01`.`ads_platform_core_index` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;
'DescribeRelation false, [col_name#9, data_type#10, comment#11]
+- 'UnresolvedTableOrView [gd01, ads_platform_core_index], DESCRIBE TABLE, true
，跳过继续处理其他表
2025-08-09 08:50:29,971 - INFO - Closing down clientserver connection
2025-08-09 08:53:05,771 - INFO - 准备同步 2 张表: ads_product_efficiency_monitor, ads_product_range_analysis
2025-08-09 08:53:09,219 - WARNING - 表 ads_product_efficiency_monitor 中发现重复列: dt，已跳过
2025-08-09 08:53:09,356 - INFO - 表 ads_product_efficiency_monitor 结构同步成功
2025-08-09 08:53:11,715 - INFO - 表 ads_product_efficiency_monitor 数据同步完成，记录数: 1
2025-08-09 08:53:11,819 - WARNING - 表 ads_product_range_analysis 中发现重复列: dt，已跳过
2025-08-09 08:53:11,910 - INFO - 表 ads_product_range_analysis 结构同步成功
2025-08-09 08:53:12,625 - INFO - 表 ads_product_range_analysis 数据同步完成，记录数: 93
2025-08-09 08:53:12,762 - INFO - Closing down clientserver connection
2025-08-12 20:36:18,380 - ERROR - KeyboardInterrupt while sending command.
Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "D:\ANACONDA\envs\offline-pyspark\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "D:\ANACONDA\envs\offline-pyspark\lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 1477, in info
    self._log(INFO, msg, args, **kwargs)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 1624, in _log
    self.handle(record)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 1634, in handle
    self.callHandlers(record)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 1696, in callHandlers
    hdlr.handle(record)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 968, in handle
    self.emit(record)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 1218, in emit
    StreamHandler.emit(self, record)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 1100, in emit
    msg = self.format(record)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 943, in format
    return fmt.format(record)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 686, in format
    record.exc_text = self.formatException(record.exc_info)
  File "D:\ANACONDA\envs\offline-pyspark\lib\logging\__init__.py", line 636, in formatException
    traceback.print_exception(ei[0], ei[1], tb, None, sio)
  File "D:\ANACONDA\envs\offline-pyspark\lib\traceback.py", line 119, in print_exception
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
  File "D:\ANACONDA\envs\offline-pyspark\lib\traceback.py", line 502, in __init__
    self.stack = StackSummary.extract(
  File "D:\ANACONDA\envs\offline-pyspark\lib\traceback.py", line 383, in extract
    f.line
  File "D:\ANACONDA\envs\offline-pyspark\lib\traceback.py", line 306, in line
    self._line = linecache.getline(self.filename, self.lineno)
  File "D:\ANACONDA\envs\offline-pyspark\lib\linecache.py", line 30, in getline
    lines = getlines(filename, module_globals)
  File "D:\ANACONDA\envs\offline-pyspark\lib\linecache.py", line 46, in getlines
    return updatecache(filename, module_globals)
  File "D:\ANACONDA\envs\offline-pyspark\lib\linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "D:\ANACONDA\envs\offline-pyspark\lib\tokenize.py", line 394, in open
    buffer = _builtin_open(filename, 'rb')
KeyboardInterrupt
2025-08-12 20:36:18,463 - INFO - Closing down clientserver connection
2025-08-12 20:36:18,485 - INFO - Closing down clientserver connection
2025-08-12 20:36:18,495 - INFO - Closing down clientserver connection
2025-08-12 20:36:33,675 - INFO - 发现 28 张待同步表
2025-08-12 20:36:34,484 - WARNING - 表 ads_instore_path_analysis_pc 中发现重复列: dt，已跳过
2025-08-12 20:36:34,601 - INFO - 表 ads_instore_path_analysis_pc 结构同步成功
2025-08-12 20:36:37,208 - INFO - 表 ads_instore_path_analysis_pc 数据同步完成，记录数: 29
2025-08-12 20:36:37,301 - WARNING - 表 ads_instore_path_analysis_wireless 中发现重复列: dt，已跳过
2025-08-12 20:36:37,375 - INFO - 表 ads_instore_path_analysis_wireless 结构同步成功
2025-08-12 20:36:37,556 - INFO - 表 ads_instore_path_analysis_wireless 数据同步完成，记录数: 29
2025-08-12 20:36:37,641 - WARNING - 表 ads_pc_entry_indicator 中发现重复列: dt，已跳过
2025-08-12 20:36:37,720 - INFO - 表 ads_pc_entry_indicator 结构同步成功
2025-08-12 20:36:37,939 - INFO - 表 ads_pc_entry_indicator 数据同步完成，记录数: 9
2025-08-12 20:36:38,020 - WARNING - 表 ads_pc_entry_page_type_top20 中发现重复列: dt，已跳过
2025-08-12 20:36:38,089 - INFO - 表 ads_pc_entry_page_type_top20 结构同步成功
2025-08-12 20:36:38,793 - INFO - 表 ads_pc_entry_page_type_top20 数据同步完成，记录数: 9
2025-08-12 20:36:38,886 - WARNING - 表 ads_shop_page_analysis_pc 中发现重复列: dt，已跳过
2025-08-12 20:36:38,975 - INFO - 表 ads_shop_page_analysis_pc 结构同步成功
2025-08-12 20:36:39,764 - INFO - 表 ads_shop_page_analysis_pc 数据同步完成，记录数: 1005
2025-08-12 20:36:39,987 - WARNING - 表 ads_shop_page_analysis_wireless 中发现重复列: dt，已跳过
2025-08-12 20:36:40,091 - INFO - 表 ads_shop_page_analysis_wireless 结构同步成功
2025-08-12 20:36:40,856 - INFO - 表 ads_shop_page_analysis_wireless 数据同步完成，记录数: 1005
2025-08-12 20:36:40,963 - WARNING - 表 ads_wireless_entry_indicator 中发现重复列: dt，已跳过
2025-08-12 20:36:41,029 - INFO - 表 ads_wireless_entry_indicator 结构同步成功
2025-08-12 20:36:41,205 - INFO - 表 ads_wireless_entry_indicator 数据同步完成，记录数: 9
2025-08-12 20:36:41,285 - WARNING - 表 dwd_instore_path_pc 中发现重复列: dt，已跳过
2025-08-12 20:36:41,393 - INFO - 表 dwd_instore_path_pc 结构同步成功
2025-08-12 20:37:05,038 - INFO - 表 dwd_instore_path_pc 数据同步完成，记录数: 38501
2025-08-12 20:37:05,130 - WARNING - 表 dwd_instore_path_wireless 中发现重复列: dt，已跳过
2025-08-12 20:37:05,202 - INFO - 表 dwd_instore_path_wireless 结构同步成功
2025-08-12 20:37:21,328 - INFO - 表 dwd_instore_path_wireless 数据同步完成，记录数: 25829
2025-08-12 20:37:21,429 - WARNING - 表 dwd_pc_entry_detail 中发现重复列: dt，已跳过
2025-08-12 20:37:21,526 - INFO - 表 dwd_pc_entry_detail 结构同步成功
2025-08-12 20:38:27,117 - INFO - Closing down clientserver connection
